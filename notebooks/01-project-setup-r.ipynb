{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Project Setup & ML Demo (R Version)\n",
    "\n",
    "**WISE Workshop | Addis Ababa, Feb 2026**\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Set up a reproducible analysis project using tidymodels\n",
    "2. Explore the workshop dataset\n",
    "3. **See overfitting in action** with a sine wave demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "::: {.callout-important}\n",
    "## Google Colab R Runtime\n",
    "Make sure you're using the R runtime: **Runtime → Change runtime type → R**\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "cat(\"R version:\", R.version.string, \"\\n\")\n",
    "cat(\"Environment:\", ifelse(Sys.getenv(\"COLAB_RELEASE_TAG\") != \"\", \"Colab\", \"Local\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tidymodels if not already installed (run once)\n",
    "# This may take 1-2 minutes in Colab\n",
    "if (!require(\"tidymodels\", quietly = TRUE)) {\n",
    "  install.packages(\"tidymodels\")\n",
    "}\n",
    "\n",
    "# Load packages\n",
    "library(tidymodels)\n",
    "library(tidyverse)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set.seed(42)\n",
    "\n",
    "# Settings\n",
    "theme_set(theme_minimal())\n",
    "\n",
    "cat(\"Packages loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load the Workshop Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load supply chain data from GitHub\n",
    "url <- \"https://raw.githubusercontent.com/sysylvia/ethiopia-ds-workshop-2026/main/data/supply-chain-sample.csv\"\n",
    "\n",
    "tryCatch({\n",
    "  df <- read_csv(url, show_col_types = FALSE)\n",
    "  cat(\"Data loaded successfully! Shape:\", nrow(df), \"rows x\", ncol(df), \"columns\\n\")\n",
    "}, error = function(e) {\n",
    "  # If URL not available, create sample data\n",
    "  cat(\"Creating sample data for demonstration...\\n\")\n",
    "  set.seed(42)\n",
    "  n_rows <- 500\n",
    "  \n",
    "  df <<- tibble(\n",
    "    facility_id = sample(c('ETH001', 'ETH002', 'ETH003', 'ETH004', 'ETH005'), n_rows, replace = TRUE),\n",
    "    region = sample(c('Addis Ababa', 'Oromia', 'Amhara', 'SNNP', 'Tigray'), n_rows, replace = TRUE),\n",
    "    facility_type = sample(c('Hospital', 'Health Center', 'Clinic'), n_rows, replace = TRUE, \n",
    "                          prob = c(0.2, 0.5, 0.3)),\n",
    "    date = format(seq(as.Date('2023-01-01'), length.out = n_rows, by = 'day'), '%Y-%m'),\n",
    "    medication_class = sample(c('Antibiotics', 'Antimalarials', 'Chronic Disease', 'Vaccines', 'Other'), \n",
    "                             n_rows, replace = TRUE),\n",
    "    demand = rpois(n_rows, 100) + sample(0:49, n_rows, replace = TRUE),\n",
    "    stock_level = rpois(n_rows, 150),\n",
    "    lead_time_days = sample(c(7, 14, 21, 30), n_rows, replace = TRUE, prob = c(0.3, 0.4, 0.2, 0.1))\n",
    "  )\n",
    "  cat(\"Sample data created! Shape:\", nrow(df), \"rows x\", ncol(df), \"columns\\n\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "glimpse(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical variables\n",
    "cat(\"Region:\\n\")\n",
    "print(table(df$region))\n",
    "\n",
    "cat(\"\\nFacility Type:\\n\")\n",
    "print(table(df$facility_type))\n",
    "\n",
    "cat(\"\\nMedication Class:\\n\")\n",
    "print(table(df$medication_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Initial Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of demand\n",
    "ggplot(df, aes(x = demand)) +\n",
    "  geom_histogram(bins = 30, fill = \"steelblue\", color = \"black\", alpha = 0.7) +\n",
    "  labs(\n",
    "    x = \"Demand (units)\",\n",
    "    y = \"Frequency\",\n",
    "    title = \"Distribution of Demand\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand by region\n",
    "df %>%\n",
    "  group_by(region) %>%\n",
    "  summarize(avg_demand = mean(demand), .groups = \"drop\") %>%\n",
    "  ggplot(aes(x = avg_demand, y = reorder(region, avg_demand))) +\n",
    "  geom_col(fill = \"steelblue\", alpha = 0.7) +\n",
    "  labs(\n",
    "    x = \"Average Demand\",\n",
    "    y = \"Region\",\n",
    "    title = \"Average Demand by Region\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand by facility type\n",
    "ggplot(df, aes(x = facility_type, y = demand)) +\n",
    "  geom_boxplot(fill = \"steelblue\", alpha = 0.7) +\n",
    "  labs(\n",
    "    x = \"Facility Type\",\n",
    "    y = \"Demand\",\n",
    "    title = \"Demand Distribution by Facility Type\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Understanding Overfitting with the Sine Wave\n",
    "\n",
    "Now let's see the **bias-variance tradeoff** in action! We'll generate noisy data from a sine wave and try to fit polynomials of increasing complexity.\n",
    "\n",
    "**Key questions:**\n",
    "- When does the model fit too much noise?\n",
    "- Why does training error alone mislead us?\n",
    "- How do we find the right complexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sine wave data with noise\n",
    "set.seed(42)\n",
    "\n",
    "n <- 30  # number of data points\n",
    "X <- sort(runif(n, 0, 2 * pi))  # random x values\n",
    "y_true <- sin(X)  # the true underlying function\n",
    "y <- y_true + rnorm(n, 0, 0.3)  # add noise\n",
    "\n",
    "# Create data frame\n",
    "sine_data <- tibble(x = X, y = y, y_true = y_true)\n",
    "\n",
    "cat(\"Generated\", n, \"noisy observations from sin(x)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data and true function\n",
    "x_smooth <- seq(0, 2 * pi, length.out = 100)\n",
    "smooth_df <- tibble(x = x_smooth, y = sin(x_smooth))\n",
    "\n",
    "ggplot() +\n",
    "  geom_line(data = smooth_df, aes(x = x, y = y), \n",
    "            color = \"red\", linetype = \"dashed\", linewidth = 1) +\n",
    "  geom_point(data = sine_data, aes(x = x, y = y), \n",
    "             size = 3, color = \"blue\", shape = 21, fill = \"blue\") +\n",
    "  labs(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    title = \"The Challenge: Recover the True Pattern from Noisy Data\",\n",
    "    subtitle = \"Red dashed = True function sin(x), Blue points = Observed data (noisy)\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Polynomials of Increasing Degree\n",
    "\n",
    "Let's fit polynomials with degrees 1 (linear), 3, 10, and 20 to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit polynomials of different degrees\n",
    "degrees <- c(1, 3, 10, 20)\n",
    "colors <- c(\"green\", \"orange\", \"purple\", \"red\")\n",
    "\n",
    "# Create base plot\n",
    "p <- ggplot() +\n",
    "  geom_point(data = sine_data, aes(x = x, y = y), \n",
    "             size = 3, color = \"blue\", shape = 21, fill = \"blue\") +\n",
    "  geom_line(data = smooth_df, aes(x = x, y = y), \n",
    "            color = \"red\", linetype = \"dashed\", alpha = 0.5, linewidth = 1)\n",
    "\n",
    "# Fit and add each polynomial\n",
    "results <- list()\n",
    "for (i in seq_along(degrees)) {\n",
    "  degree <- degrees[i]\n",
    "  \n",
    "  # Fit polynomial using lm with poly()\n",
    "  model <- lm(y ~ poly(x, degree, raw = TRUE), data = sine_data)\n",
    "  \n",
    "  # Predictions on smooth x\n",
    "  pred_df <- tibble(\n",
    "    x = x_smooth,\n",
    "    y_pred = predict(model, newdata = tibble(x = x_smooth))\n",
    "  )\n",
    "  \n",
    "  # Training MSE\n",
    "  train_pred <- predict(model, newdata = sine_data)\n",
    "  train_mse <- mean((sine_data$y - train_pred)^2)\n",
    "  \n",
    "  results[[i]] <- list(degree = degree, mse = train_mse, pred_df = pred_df, color = colors[i])\n",
    "  \n",
    "  # Add to plot\n",
    "  p <- p + geom_line(data = pred_df, aes(x = x, y = y_pred), \n",
    "                     color = colors[i], linewidth = 1)\n",
    "}\n",
    "\n",
    "# Print MSE results\n",
    "cat(\"Training MSE by polynomial degree:\\n\")\n",
    "for (r in results) {\n",
    "  cat(sprintf(\"  Degree %2d: MSE = %.3f\\n\", r$degree, r$mse))\n",
    "}\n",
    "\n",
    "p + \n",
    "  labs(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    title = \"Polynomial Fits of Increasing Complexity\",\n",
    "    subtitle = \"Green=Deg1, Orange=Deg3, Purple=Deg10, Red=Deg20\"\n",
    "  ) +\n",
    "  ylim(-2, 2) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice?\n",
    "\n",
    "- **Degree 1 (green)**: Too simple! Misses the curve entirely.\n",
    "- **Degree 3 (orange)**: Captures the sine pattern reasonably well.\n",
    "- **Degree 10+ (purple, red)**: Starts wiggling through individual points.\n",
    "\n",
    "**But look at the training MSE!** Higher-degree polynomials have *lower* training error. Does that mean they're better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train vs Test Error: The Real Test\n",
    "\n",
    "Let's split our data and see what happens on **held-out test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using rsample: 70% train, 30% test\n",
    "set.seed(42)\n",
    "data_split <- initial_split(sine_data, prop = 0.7)\n",
    "train_data <- training(data_split)\n",
    "test_data <- testing(data_split)\n",
    "\n",
    "cat(\"Training samples:\", nrow(train_data), \"\\n\")\n",
    "cat(\"Test samples:\", nrow(test_data), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train vs test error across polynomial degrees\n",
    "degrees_to_test <- 1:15\n",
    "train_errors <- numeric(length(degrees_to_test))\n",
    "test_errors <- numeric(length(degrees_to_test))\n",
    "\n",
    "for (i in seq_along(degrees_to_test)) {\n",
    "  degree <- degrees_to_test[i]\n",
    "  \n",
    "  # Fit model on training data\n",
    "  model <- lm(y ~ poly(x, degree, raw = TRUE), data = train_data)\n",
    "  \n",
    "  # Calculate errors\n",
    "  train_pred <- predict(model, newdata = train_data)\n",
    "  test_pred <- predict(model, newdata = test_data)\n",
    "  \n",
    "  train_errors[i] <- mean((train_data$y - train_pred)^2)\n",
    "  test_errors[i] <- mean((test_data$y - test_pred)^2)\n",
    "}\n",
    "\n",
    "# Create data frame for plotting\n",
    "error_df <- tibble(\n",
    "  degree = rep(degrees_to_test, 2),\n",
    "  error = c(train_errors, test_errors),\n",
    "  type = rep(c(\"Training Error\", \"Test Error\"), each = length(degrees_to_test))\n",
    ")\n",
    "\n",
    "# Find optimal degree\n",
    "optimal_degree <- degrees_to_test[which.min(test_errors)]\n",
    "\n",
    "# Plot\n",
    "ggplot(error_df, aes(x = degree, y = error, color = type, shape = type)) +\n",
    "  geom_line(linewidth = 1) +\n",
    "  geom_point(size = 3) +\n",
    "  geom_vline(xintercept = optimal_degree, linetype = \"dashed\", color = \"green\", alpha = 0.7) +\n",
    "  annotate(\"text\", x = optimal_degree + 1, y = max(error_df$error) * 0.8, \n",
    "           label = paste(\"Optimal: Degree\", optimal_degree), hjust = 0) +\n",
    "  scale_y_log10() +\n",
    "  scale_color_manual(values = c(\"Training Error\" = \"blue\", \"Test Error\" = \"red\")) +\n",
    "  labs(\n",
    "    x = \"Polynomial Degree\",\n",
    "    y = \"Mean Squared Error (log scale)\",\n",
    "    title = \"The Bias-Variance Tradeoff in Action\",\n",
    "    color = \"\",\n",
    "    shape = \"\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(legend.position = \"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Aha\" Moment!\n",
    "\n",
    "**Training error** keeps decreasing as we add complexity.\n",
    "\n",
    "**Test error** eventually starts INCREASING!\n",
    "\n",
    "This is **overfitting**: the model memorizes training data (including noise) and fails to generalize.\n",
    "\n",
    "---\n",
    "\n",
    "**Key lessons:**\n",
    "1. Training error alone is misleading\n",
    "2. We need held-out test data to evaluate models honestly\n",
    "3. More complex isn't always better\n",
    "4. The optimal complexity balances bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation: A Better Approach\n",
    "\n",
    "Instead of a single train/test split, let's use **cross-validation** to get more stable estimates.\n",
    "\n",
    "In tidymodels, we use `vfold_cv()` for k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to find optimal degree\n",
    "degrees_to_test <- 1:11\n",
    "cv_scores <- numeric(length(degrees_to_test))\n",
    "\n",
    "# Create 5-fold cross-validation folds\n",
    "set.seed(42)\n",
    "folds <- vfold_cv(sine_data, v = 5)\n",
    "\n",
    "for (i in seq_along(degrees_to_test)) {\n",
    "  degree <- degrees_to_test[i]\n",
    "  fold_mses <- numeric(5)\n",
    "  \n",
    "  # Manual CV loop (for educational purposes)\n",
    "  for (j in 1:5) {\n",
    "    # Get train/test for this fold\n",
    "    train_fold <- analysis(folds$splits[[j]])\n",
    "    test_fold <- assessment(folds$splits[[j]])\n",
    "    \n",
    "    # Fit model\n",
    "    model <- lm(y ~ poly(x, degree, raw = TRUE), data = train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    pred <- predict(model, newdata = test_fold)\n",
    "    fold_mses[j] <- mean((test_fold$y - pred)^2)\n",
    "  }\n",
    "  \n",
    "  cv_scores[i] <- mean(fold_mses)\n",
    "}\n",
    "\n",
    "# Find optimal degree\n",
    "optimal_cv_degree <- degrees_to_test[which.min(cv_scores)]\n",
    "\n",
    "# Plot\n",
    "cv_df <- tibble(degree = degrees_to_test, mse = cv_scores)\n",
    "\n",
    "ggplot(cv_df, aes(x = degree, y = mse)) +\n",
    "  geom_line(color = \"darkgreen\", linewidth = 1) +\n",
    "  geom_point(color = \"darkgreen\", size = 3) +\n",
    "  geom_vline(xintercept = optimal_cv_degree, linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n",
    "  annotate(\"text\", x = optimal_cv_degree + 0.5, y = max(cv_df$mse) * 0.8,\n",
    "           label = paste(\"CV Optimal: Degree\", optimal_cv_degree), hjust = 0, color = \"red\") +\n",
    "  labs(\n",
    "    x = \"Polynomial Degree\",\n",
    "    y = \"Cross-Validation MSE\",\n",
    "    title = \"Using Cross-Validation to Select Model Complexity\"\n",
    "  ) +\n",
    "  theme_minimal()\n",
    "\n",
    "cat(\"\\n✓ Cross-validation suggests degree\", optimal_cv_degree, \"is optimal!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Day 2: Regularization\n",
    "\n",
    "Instead of choosing polynomial degree, tomorrow we'll learn a more elegant approach:\n",
    "\n",
    "**LASSO and Ridge regression** add penalties that automatically constrain model complexity!\n",
    "\n",
    "```\n",
    "Today:     Choose degree to control complexity\n",
    "Tomorrow:  Use regularization penalty (λ) to control complexity\n",
    "```\n",
    "\n",
    "The same principle applies: **constrain complexity to prevent overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Save Your Work\n",
    "\n",
    "Don't forget to save a copy of this notebook to your Google Drive!\n",
    "\n",
    "**File > Save a copy in Drive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "1. ✅ Set up your R environment with tidymodels\n",
    "2. ✅ Loaded and explored the workshop dataset\n",
    "3. ✅ Created initial visualizations with ggplot2\n",
    "4. ✅ **Saw overfitting in action** with the sine wave demo\n",
    "5. ✅ Learned why train/test splits and cross-validation matter\n",
    "\n",
    "**Key takeaway**: Complex models that fit training data perfectly often fail on new data. We need validation strategies to find the right balance.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Day 2 - Regularization (LASSO, Ridge) & Tree-Based Methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
