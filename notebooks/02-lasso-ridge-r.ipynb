{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Regularized Regression (LASSO, Ridge, Elastic Net) - R Version\n",
    "\n",
    "**WISE Workshop | Addis Ababa, Feb 2026**\n",
    "\n",
    "In this notebook, you'll apply regularization techniques to prevent overfitting in supply chain demand prediction using the tidymodels framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "::: {.callout-important}\n",
    "## Google Colab R Runtime\n",
    "Make sure you're using the R runtime: **Runtime -> Change runtime type -> R**\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed (run once in Colab)\n",
    "if (!require(\"tidymodels\", quietly = TRUE)) {\n",
    "  install.packages(c(\"tidymodels\", \"glmnet\"))\n",
    "}\n",
    "\n",
    "# Load packages\n",
    "library(tidymodels)\n",
    "library(tidyverse)\n",
    "library(glmnet)\n",
    "\n",
    "# Settings\n",
    "set.seed(42)\n",
    "theme_set(theme_minimal())\n",
    "\n",
    "cat(\"Packages loaded!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "We'll create a dataset with many features to see regularization in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample supply chain data with many features\n",
    "set.seed(42)\n",
    "n_rows <- 1000\n",
    "\n",
    "# Generate dates and basic features\n",
    "dates <- seq(as.Date('2023-01-01'), by = 'day', length.out = n_rows)\n",
    "regions <- sample(c('Addis Ababa', 'Oromia', 'Amhara', 'SNNP', 'Tigray'), n_rows, replace = TRUE)\n",
    "facility_types <- sample(c('Hospital', 'Health Center', 'Clinic'), n_rows, replace = TRUE, \n",
    "                        prob = c(0.2, 0.5, 0.3))\n",
    "\n",
    "# Create demand with clear patterns\n",
    "base_demand <- 100\n",
    "facility_effect <- ifelse(facility_types == 'Hospital', 80,\n",
    "                         ifelse(facility_types == 'Health Center', 30, 0))\n",
    "region_effect <- ifelse(regions == 'Addis Ababa', 50,\n",
    "                       ifelse(regions == 'Oromia', 20, 0))\n",
    "day_of_year <- as.numeric(format(dates, '%j'))\n",
    "seasonal_effect <- 25 * sin(2 * pi * day_of_year / 365)\n",
    "noise <- rnorm(n_rows, 0, 15)\n",
    "\n",
    "demand <- pmax(base_demand + facility_effect + region_effect + seasonal_effect + noise, 10)\n",
    "\n",
    "df <- tibble(\n",
    "  date = dates,\n",
    "  region = regions,\n",
    "  facility_type = facility_types,\n",
    "  demand = as.integer(demand)\n",
    ")\n",
    "\n",
    "cat(\"Data shape:\", nrow(df), \"rows x\", ncol(df), \"columns\\n\")\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create MANY features (some useful, some noise)\n",
    "\n",
    "df <- df %>%\n",
    "  mutate(\n",
    "    # Time-based features\n",
    "    month = month(date),\n",
    "    day_of_week = wday(date),\n",
    "    quarter = quarter(date),\n",
    "    day_of_year = yday(date),\n",
    "    week_of_year = isoweek(date),\n",
    "    is_weekend = as.integer(day_of_week %in% c(1, 7)),\n",
    "    \n",
    "    # Cyclical encoding for month (sine/cosine)\n",
    "    month_sin = sin(2 * pi * month / 12),\n",
    "    month_cos = cos(2 * pi * month / 12),\n",
    "    \n",
    "    # Noise features (to see if LASSO eliminates them)\n",
    "    noise_0 = rnorm(n()),\n",
    "    noise_1 = rnorm(n()),\n",
    "    noise_2 = rnorm(n()),\n",
    "    noise_3 = rnorm(n()),\n",
    "    noise_4 = rnorm(n())\n",
    "  )\n",
    "\n",
    "cat(\"After feature engineering:\", nrow(df), \"rows x\", ncol(df), \"columns\\n\")\n",
    "cat(\"\\nNumeric features:\", names(df)[sapply(df, is.numeric)], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target using recipes\n",
    "# We'll use a recipe to handle categorical encoding and scaling\n",
    "\n",
    "# Split data first\n",
    "set.seed(42)\n",
    "data_split <- initial_split(df, prop = 0.8)\n",
    "train_data <- training(data_split)\n",
    "test_data <- testing(data_split)\n",
    "\n",
    "cat(\"Training samples:\", nrow(train_data), \"\\n\")\n",
    "cat(\"Test samples:\", nrow(test_data), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a recipe for preprocessing\n",
    "demand_recipe <- recipe(demand ~ ., data = train_data) %>%\n",
    "  # Remove date (not a predictor)\n",
    "  step_rm(date) %>%\n",
    "  # Create dummy variables for categorical features\n",
    "  step_dummy(all_nominal_predictors()) %>%\n",
    "  # IMPORTANT: Normalize features for regularization\n",
    "  step_normalize(all_numeric_predictors())\n",
    "\n",
    "# Prep the recipe to see what features we have\n",
    "prepped_recipe <- prep(demand_recipe)\n",
    "train_processed <- bake(prepped_recipe, new_data = NULL)\n",
    "\n",
    "cat(\"Number of features after preprocessing:\", ncol(train_processed) - 1, \"\\n\")\n",
    "cat(\"Feature names:\\n\")\n",
    "print(names(train_processed)[names(train_processed) != \"demand\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Ridge Regression\n",
    "\n",
    "Ridge regression adds an L2 penalty: lambda * sum(beta_j^2)\n",
    "\n",
    "This **shrinks** all coefficients toward zero but never sets them exactly to zero.\n",
    "\n",
    "In tidymodels, we use `linear_reg()` with `mixture = 0` for Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Ridge with different penalty (lambda) values using glmnet directly for visualization\n",
    "# Prepare data matrices\n",
    "X_train <- train_processed %>% select(-demand) %>% as.matrix()\n",
    "y_train <- train_processed$demand\n",
    "\n",
    "# Fit Ridge path\n",
    "ridge_fit <- glmnet(X_train, y_train, alpha = 0)  # alpha = 0 is Ridge\n",
    "\n",
    "# Visualize coefficient shrinkage with Ridge\n",
    "plot(ridge_fit, xvar = \"lambda\", label = TRUE)\n",
    "title(\"Ridge Regression: Coefficient Paths\\n(All coefficients shrink, but none reach zero)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to find optimal penalty with tidymodels\n",
    "\n",
    "# Define the Ridge model specification (mixture = 0)\n",
    "ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%\n",
    "  set_engine(\"glmnet\")\n",
    "\n",
    "# Create workflow\n",
    "ridge_workflow <- workflow() %>%\n",
    "  add_recipe(demand_recipe) %>%\n",
    "  add_model(ridge_spec)\n",
    "\n",
    "# Create cross-validation folds\n",
    "set.seed(42)\n",
    "cv_folds <- vfold_cv(train_data, v = 5)\n",
    "\n",
    "# Define penalty grid\n",
    "penalty_grid <- tibble(penalty = 10^seq(-3, 3, length.out = 50))\n",
    "\n",
    "# Tune the model\n",
    "ridge_tune <- tune_grid(\n",
    "  ridge_workflow,\n",
    "  resamples = cv_folds,\n",
    "  grid = penalty_grid,\n",
    "  metrics = metric_set(rmse, rsq)\n",
    ")\n",
    "\n",
    "# Show best results\n",
    "cat(\"Best Ridge penalty (by RMSE):\\n\")\n",
    "show_best(ridge_tune, metric = \"rmse\", n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and evaluate Ridge model\n",
    "best_ridge <- select_best(ridge_tune, metric = \"rmse\")\n",
    "cat(\"Optimal penalty:\", best_ridge$penalty, \"\\n\")\n",
    "\n",
    "# Finalize workflow with best parameters\n",
    "final_ridge <- ridge_workflow %>%\n",
    "  finalize_workflow(best_ridge) %>%\n",
    "  fit(data = train_data)\n",
    "\n",
    "# Evaluate on test set\n",
    "ridge_pred <- predict(final_ridge, test_data) %>%\n",
    "  bind_cols(test_data %>% select(demand))\n",
    "\n",
    "ridge_metrics <- ridge_pred %>%\n",
    "  metrics(truth = demand, estimate = .pred)\n",
    "\n",
    "ridge_rmse <- ridge_metrics %>% filter(.metric == \"rmse\") %>% pull(.estimate)\n",
    "ridge_rsq <- ridge_metrics %>% filter(.metric == \"rsq\") %>% pull(.estimate)\n",
    "\n",
    "cat(\"Ridge Test RMSE:\", round(ridge_rmse, 2), \"\\n\")\n",
    "cat(\"Ridge Test R-squared:\", round(ridge_rsq, 3), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LASSO Regression\n",
    "\n",
    "LASSO adds an L1 penalty: lambda * sum(|beta_j|)\n",
    "\n",
    "This **shrinks** coefficients AND can set them **exactly to zero** (feature selection!).\n",
    "\n",
    "In tidymodels, we use `linear_reg()` with `mixture = 1` for LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LASSO path for visualization\n",
    "lasso_fit <- glmnet(X_train, y_train, alpha = 1)  # alpha = 1 is LASSO\n",
    "\n",
    "# Visualize coefficient paths with LASSO\n",
    "plot(lasso_fit, xvar = \"lambda\", label = TRUE)\n",
    "title(\"LASSO Regression: Coefficient Paths\\n(Watch coefficients go to EXACTLY zero!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to find optimal penalty for LASSO with tidymodels\n",
    "\n",
    "# Define the LASSO model specification (mixture = 1)\n",
    "lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n",
    "  set_engine(\"glmnet\")\n",
    "\n",
    "# Create workflow\n",
    "lasso_workflow <- workflow() %>%\n",
    "  add_recipe(demand_recipe) %>%\n",
    "  add_model(lasso_spec)\n",
    "\n",
    "# Tune the model (use same folds and grid)\n",
    "lasso_tune <- tune_grid(\n",
    "  lasso_workflow,\n",
    "  resamples = cv_folds,\n",
    "  grid = penalty_grid,\n",
    "  metrics = metric_set(rmse, rsq)\n",
    ")\n",
    "\n",
    "# Show best results\n",
    "cat(\"Best LASSO penalty (by RMSE):\\n\")\n",
    "show_best(lasso_tune, metric = \"rmse\", n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and evaluate LASSO model\n",
    "best_lasso <- select_best(lasso_tune, metric = \"rmse\")\n",
    "cat(\"Optimal penalty:\", best_lasso$penalty, \"\\n\")\n",
    "\n",
    "# Finalize workflow with best parameters\n",
    "final_lasso <- lasso_workflow %>%\n",
    "  finalize_workflow(best_lasso) %>%\n",
    "  fit(data = train_data)\n",
    "\n",
    "# Evaluate on test set\n",
    "lasso_pred <- predict(final_lasso, test_data) %>%\n",
    "  bind_cols(test_data %>% select(demand))\n",
    "\n",
    "lasso_metrics <- lasso_pred %>%\n",
    "  metrics(truth = demand, estimate = .pred)\n",
    "\n",
    "lasso_rmse <- lasso_metrics %>% filter(.metric == \"rmse\") %>% pull(.estimate)\n",
    "lasso_rsq <- lasso_metrics %>% filter(.metric == \"rsq\") %>% pull(.estimate)\n",
    "\n",
    "cat(\"LASSO Test RMSE:\", round(lasso_rmse, 2), \"\\n\")\n",
    "cat(\"LASSO Test R-squared:\", round(lasso_rsq, 3), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See which features LASSO selected (non-zero coefficients)\n",
    "lasso_coefs <- final_lasso %>%\n",
    "  extract_fit_parsnip() %>%\n",
    "  tidy()\n",
    "\n",
    "# Count zero and non-zero coefficients (excluding intercept)\n",
    "lasso_coefs_no_int <- lasso_coefs %>% filter(term != \"(Intercept)\")\n",
    "n_selected <- sum(lasso_coefs_no_int$estimate != 0)\n",
    "n_eliminated <- sum(lasso_coefs_no_int$estimate == 0)\n",
    "\n",
    "cat(\"Features SELECTED by LASSO:\", n_selected, \"\\n\")\n",
    "cat(\"Features ELIMINATED by LASSO:\", n_eliminated, \"\\n\")\n",
    "cat(\"\\nSelected features:\\n\")\n",
    "lasso_coefs_no_int %>%\n",
    "  filter(estimate != 0) %>%\n",
    "  arrange(desc(abs(estimate))) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LASSO feature selection\n",
    "lasso_coefs_plot <- lasso_coefs_no_int %>%\n",
    "  mutate(\n",
    "    selected = estimate != 0,\n",
    "    term = fct_reorder(term, abs(estimate))\n",
    "  )\n",
    "\n",
    "ggplot(lasso_coefs_plot, aes(x = estimate, y = term, fill = selected)) +\n",
    "  geom_col() +\n",
    "  geom_vline(xintercept = 0, color = \"black\", linewidth = 0.5) +\n",
    "  scale_fill_manual(values = c(\"TRUE\" = \"forestgreen\", \"FALSE\" = \"gray70\"),\n",
    "                    labels = c(\"Eliminated\", \"Selected\")) +\n",
    "  labs(\n",
    "    x = \"Coefficient Value\",\n",
    "    y = \"Feature\",\n",
    "    title = \"LASSO Feature Selection\",\n",
    "    subtitle = \"Green = Selected, Gray = Eliminated\",\n",
    "    fill = \"\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.y = element_text(size = 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Look at which features LASSO selected:\n",
    "- Did it keep the region and facility features (which have real effects)?\n",
    "- Did it eliminate the noise features?\n",
    "- What about the cyclical month encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Elastic Net\n",
    "\n",
    "Elastic Net combines L1 and L2 penalties:\n",
    "\n",
    "Loss = MSE + lambda1 * sum(|beta_j|) + lambda2 * sum(beta_j^2)\n",
    "\n",
    "This gives you the best of both worlds: feature selection (L1) + grouped selection of correlated features (L2).\n",
    "\n",
    "In tidymodels, `mixture` controls the blend: 0 = Ridge, 1 = LASSO, 0.5 = balanced mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Elastic Net with cross-validation, tuning both penalty and mixture\n",
    "\n",
    "# Define the Elastic Net model specification\n",
    "elastic_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%\n",
    "  set_engine(\"glmnet\")\n",
    "\n",
    "# Create workflow\n",
    "elastic_workflow <- workflow() %>%\n",
    "  add_recipe(demand_recipe) %>%\n",
    "  add_model(elastic_spec)\n",
    "\n",
    "# Define grid for both penalty and mixture\n",
    "elastic_grid <- grid_regular(\n",
    "  penalty(range = c(-3, 1)),\n",
    "  mixture(range = c(0.1, 1)),\n",
    "  levels = c(30, 6)\n",
    ")\n",
    "\n",
    "# Tune the model\n",
    "elastic_tune <- tune_grid(\n",
    "  elastic_workflow,\n",
    "  resamples = cv_folds,\n",
    "  grid = elastic_grid,\n",
    "  metrics = metric_set(rmse, rsq)\n",
    ")\n",
    "\n",
    "# Show best results\n",
    "cat(\"Best Elastic Net parameters (by RMSE):\\n\")\n",
    "show_best(elastic_tune, metric = \"rmse\", n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and evaluate Elastic Net model\n",
    "best_elastic <- select_best(elastic_tune, metric = \"rmse\")\n",
    "cat(\"Optimal penalty:\", best_elastic$penalty, \"\\n\")\n",
    "cat(\"Optimal mixture:\", best_elastic$mixture, \"\\n\")\n",
    "\n",
    "# Finalize workflow with best parameters\n",
    "final_elastic <- elastic_workflow %>%\n",
    "  finalize_workflow(best_elastic) %>%\n",
    "  fit(data = train_data)\n",
    "\n",
    "# Evaluate on test set\n",
    "elastic_pred <- predict(final_elastic, test_data) %>%\n",
    "  bind_cols(test_data %>% select(demand))\n",
    "\n",
    "elastic_metrics <- elastic_pred %>%\n",
    "  metrics(truth = demand, estimate = .pred)\n",
    "\n",
    "elastic_rmse <- elastic_metrics %>% filter(.metric == \"rmse\") %>% pull(.estimate)\n",
    "elastic_rsq <- elastic_metrics %>% filter(.metric == \"rsq\") %>% pull(.estimate)\n",
    "\n",
    "cat(\"Elastic Net Test RMSE:\", round(elastic_rmse, 2), \"\\n\")\n",
    "cat(\"Elastic Net Test R-squared:\", round(elastic_rsq, 3), \"\\n\")\n",
    "\n",
    "# Count features selected\n",
    "elastic_coefs <- final_elastic %>%\n",
    "  extract_fit_parsnip() %>%\n",
    "  tidy() %>%\n",
    "  filter(term != \"(Intercept)\")\n",
    "\n",
    "n_selected_elastic <- sum(elastic_coefs$estimate != 0)\n",
    "cat(\"Features selected by Elastic Net:\", n_selected_elastic, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS for baseline using tidymodels\n",
    "ols_spec <- linear_reg() %>%\n",
    "  set_engine(\"lm\")\n",
    "\n",
    "ols_workflow <- workflow() %>%\n",
    "  add_recipe(demand_recipe) %>%\n",
    "  add_model(ols_spec)\n",
    "\n",
    "ols_fit <- ols_workflow %>% fit(data = train_data)\n",
    "\n",
    "ols_pred <- predict(ols_fit, test_data) %>%\n",
    "  bind_cols(test_data %>% select(demand))\n",
    "\n",
    "ols_metrics <- ols_pred %>%\n",
    "  metrics(truth = demand, estimate = .pred)\n",
    "\n",
    "ols_rmse <- ols_metrics %>% filter(.metric == \"rmse\") %>% pull(.estimate)\n",
    "ols_rsq <- ols_metrics %>% filter(.metric == \"rsq\") %>% pull(.estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "total_features <- ncol(train_processed) - 1\n",
    "\n",
    "results <- tibble(\n",
    "  Model = c('OLS (Baseline)', 'Ridge', 'LASSO', 'Elastic Net'),\n",
    "  Test_RMSE = round(c(ols_rmse, ridge_rmse, lasso_rmse, elastic_rmse), 2),\n",
    "  Test_Rsq = round(c(ols_rsq, ridge_rsq, lasso_rsq, elastic_rsq), 3),\n",
    "  Features_Used = c(total_features, total_features, n_selected, n_selected_elastic)\n",
    ")\n",
    "\n",
    "cat(\"Model Comparison:\\n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "p1 <- ggplot(results, aes(x = Model, y = Test_RMSE, fill = Model)) +\n",
    "  geom_col() +\n",
    "  scale_fill_manual(values = c(\"gray60\", \"steelblue\", \"forestgreen\", \"coral\")) +\n",
    "  labs(title = \"Model Performance (Lower is Better)\", y = \"Test RMSE\") +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \"none\")\n",
    "\n",
    "p2 <- ggplot(results, aes(x = Model, y = Features_Used, fill = Model)) +\n",
    "  geom_col() +\n",
    "  scale_fill_manual(values = c(\"gray60\", \"steelblue\", \"forestgreen\", \"coral\")) +\n",
    "  labs(title = \"Model Complexity (Features Used)\", y = \"Number of Features\") +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \"none\")\n",
    "\n",
    "# Display plots side by side\n",
    "gridExtra::grid.arrange(p1, p2, ncol = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs Actual plot\n",
    "all_preds <- bind_rows(\n",
    "  ols_pred %>% mutate(Model = \"OLS\"),\n",
    "  ridge_pred %>% mutate(Model = \"Ridge\"),\n",
    "  lasso_pred %>% mutate(Model = \"LASSO\"),\n",
    "  elastic_pred %>% mutate(Model = \"Elastic Net\")\n",
    ")\n",
    "\n",
    "ggplot(all_preds, aes(x = demand, y = .pred)) +\n",
    "  geom_point(alpha = 0.5) +\n",
    "  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n",
    "  facet_wrap(~Model, nrow = 2) +\n",
    "  labs(\n",
    "    x = \"Actual Demand\",\n",
    "    y = \"Predicted Demand\",\n",
    "    title = \"Predictions vs Actual by Model\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "1. **Created features** including some noise features\n",
    "2. **Applied Ridge regression** and saw all coefficients shrink\n",
    "3. **Applied LASSO regression** and saw automatic feature selection\n",
    "4. **Applied Elastic Net** combining both approaches\n",
    "5. **Compared performance** across all methods\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Method | What it does | When to use | tidymodels mixture |\n",
    "|--------|--------------|-------------|--------------------|\n",
    "| **Ridge** | Shrinks all coefficients | Many small/medium effects | `mixture = 0` |\n",
    "| **LASSO** | Sets some coefficients to zero | Feature selection needed | `mixture = 1` |\n",
    "| **Elastic Net** | Combines both | Correlated features + selection | `mixture = 0.5` |\n",
    "\n",
    "### Connection to This Afternoon\n",
    "\n",
    "You now have a **LASSO baseline** to compare against:\n",
    "- Decision Trees\n",
    "- Random Forests (using `rand_forest()` with `ranger` engine)\n",
    "- Gradient Boosting (using `boost_tree()` with `xgboost` engine)\n",
    "\n",
    "These tree-based methods handle nonlinearities differently but also have their own regularization (max_depth, min_n, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Tree-Based Methods notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
