{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Regularized Regression (LASSO, Ridge, Elastic Net)\n",
    "\n",
    "**WISE Workshop | Addis Ababa, Feb 2026**\n",
    "\n",
    "In this notebook, you'll apply regularization techniques to prevent overfitting in supply chain demand prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Packages loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "We'll create a dataset with many features to see regularization in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample supply chain data with many features\n",
    "np.random.seed(42)\n",
    "n_rows = 1000\n",
    "\n",
    "# Generate dates and basic features\n",
    "dates = pd.date_range('2023-01-01', periods=n_rows, freq='D')\n",
    "regions = np.random.choice(['Addis Ababa', 'Oromia', 'Amhara', 'SNNP', 'Tigray'], n_rows)\n",
    "facility_types = np.random.choice(['Hospital', 'Health Center', 'Clinic'], n_rows, p=[0.2, 0.5, 0.3])\n",
    "\n",
    "# Create demand with clear patterns\n",
    "base_demand = 100\n",
    "facility_effect = np.where(facility_types == 'Hospital', 80, \n",
    "                          np.where(facility_types == 'Health Center', 30, 0))\n",
    "region_effect = np.where(regions == 'Addis Ababa', 50,\n",
    "                        np.where(regions == 'Oromia', 20, 0))\n",
    "seasonal_effect = 25 * np.sin(2 * np.pi * dates.dayofyear / 365)\n",
    "noise = np.random.normal(0, 15, n_rows)\n",
    "\n",
    "demand = base_demand + facility_effect + region_effect + seasonal_effect + noise\n",
    "demand = np.maximum(demand, 10)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'region': regions,\n",
    "    'facility_type': facility_types,\n",
    "    'demand': demand.astype(int)\n",
    "})\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create MANY features (some useful, some noise)\n",
    "\n",
    "# Time-based features\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Cyclical encoding for month (sine/cosine)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Create dummy variables for categorical features\n",
    "region_dummies = pd.get_dummies(df['region'], prefix='region', drop_first=False)\n",
    "facility_dummies = pd.get_dummies(df['facility_type'], prefix='facility', drop_first=False)\n",
    "\n",
    "# Add some noise features (to see if LASSO eliminates them)\n",
    "for i in range(5):\n",
    "    df[f'noise_{i}'] = np.random.normal(0, 1, n_rows)\n",
    "\n",
    "# Combine all features\n",
    "df = pd.concat([df, region_dummies, facility_dummies], axis=1)\n",
    "\n",
    "print(f\"After feature engineering: {df.shape}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "print([c for c in df.columns if c not in ['date', 'region', 'facility_type', 'demand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_cols = [c for c in df.columns if c not in ['date', 'region', 'facility_type', 'demand']]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['demand']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# IMPORTANT: Scale features for regularization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Ridge Regression\n",
    "\n",
    "Ridge regression adds an L2 penalty: $\\lambda \\sum \\beta_j^2$\n",
    "\n",
    "This **shrinks** all coefficients toward zero but never sets them exactly to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Ridge with different alpha (lambda) values\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "ridge_coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient shrinkage with Ridge\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i in range(ridge_coefs.shape[1]):\n",
    "    ax.plot(np.log10(alphas), ridge_coefs[:, i], linewidth=1)\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('log10(alpha)', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Ridge Regression: Coefficient Paths\\n(All coefficients shrink, but none reach zero)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to find optimal alpha\n",
    "alphas_cv = np.logspace(-3, 3, 50)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas_cv, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Optimal alpha: {ridge_cv.alpha_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "ridge_pred = ridge_cv.predict(X_test_scaled)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
    "ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "\n",
    "print(f\"Ridge Test RMSE: {ridge_rmse:.2f}\")\n",
    "print(f\"Ridge Test RÂ²: {ridge_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LASSO Regression\n",
    "\n",
    "LASSO adds an L1 penalty: $\\lambda \\sum |\\beta_j|$\n",
    "\n",
    "This **shrinks** coefficients AND can set them **exactly to zero** (feature selection!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LASSO with different alpha values\n",
    "alphas_lasso = [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10]\n",
    "lasso_coefs = []\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "\n",
    "lasso_coefs = np.array(lasso_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient paths with LASSO\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i in range(lasso_coefs.shape[1]):\n",
    "    ax.plot(np.log10(alphas_lasso), lasso_coefs[:, i], linewidth=1)\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('log10(alpha)', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('LASSO Regression: Coefficient Paths\\n(Watch coefficients go to EXACTLY zero!)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation to find optimal alpha for LASSO\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-3, 1, 50), cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Optimal alpha: {lasso_cv.alpha_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "lasso_pred = lasso_cv.predict(X_test_scaled)\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\n",
    "lasso_r2 = r2_score(y_test, lasso_pred)\n",
    "\n",
    "print(f\"LASSO Test RMSE: {lasso_rmse:.2f}\")\n",
    "print(f\"LASSO Test RÂ²: {lasso_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See which features LASSO selected (non-zero coefficients)\n",
    "lasso_coef_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lasso_cv.coef_\n",
    "})\n",
    "\n",
    "# Count zero and non-zero coefficients\n",
    "n_selected = (lasso_coef_df['coefficient'] != 0).sum()\n",
    "n_eliminated = (lasso_coef_df['coefficient'] == 0).sum()\n",
    "\n",
    "print(f\"Features SELECTED by LASSO: {n_selected}\")\n",
    "print(f\"Features ELIMINATED by LASSO: {n_eliminated}\")\n",
    "print(f\"\\nSelected features:\")\n",
    "print(lasso_coef_df[lasso_coef_df['coefficient'] != 0].sort_values('coefficient', key=abs, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LASSO feature selection\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sort by absolute coefficient value\n",
    "sorted_df = lasso_coef_df.copy()\n",
    "sorted_df['abs_coef'] = sorted_df['coefficient'].abs()\n",
    "sorted_df = sorted_df.sort_values('abs_coef', ascending=True)\n",
    "\n",
    "colors = ['green' if c != 0 else 'gray' for c in sorted_df['coefficient']]\n",
    "ax.barh(range(len(sorted_df)), sorted_df['coefficient'], color=colors)\n",
    "ax.set_yticks(range(len(sorted_df)))\n",
    "ax.set_yticklabels(sorted_df['feature'], fontsize=8)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title(f'LASSO Feature Selection\\n(Green = Selected, Gray = Eliminated)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion\n",
    "\n",
    "Look at which features LASSO selected:\n",
    "- Did it keep the region and facility features (which have real effects)?\n",
    "- Did it eliminate the noise features?\n",
    "- What about the cyclical month encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Elastic Net\n",
    "\n",
    "Elastic Net combines L1 and L2 penalties:\n",
    "\n",
    "$\\text{Loss} = \\text{MSE} + \\lambda_1 \\sum |\\beta_j| + \\lambda_2 \\sum \\beta_j^2$\n",
    "\n",
    "This gives you the best of both worlds: feature selection (L1) + grouped selection of correlated features (L2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Elastic Net with cross-validation\n",
    "# l1_ratio controls the mix: 1 = LASSO, 0 = Ridge\n",
    "elastic_cv = ElasticNetCV(\n",
    "    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n",
    "    alphas=np.logspace(-3, 1, 30),\n",
    "    cv=5,\n",
    "    max_iter=10000\n",
    ")\n",
    "elastic_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Optimal alpha: {elastic_cv.alpha_:.4f}\")\n",
    "print(f\"Optimal l1_ratio: {elastic_cv.l1_ratio_:.2f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "elastic_pred = elastic_cv.predict(X_test_scaled)\n",
    "elastic_rmse = np.sqrt(mean_squared_error(y_test, elastic_pred))\n",
    "elastic_r2 = r2_score(y_test, elastic_pred)\n",
    "\n",
    "print(f\"Elastic Net Test RMSE: {elastic_rmse:.2f}\")\n",
    "print(f\"Elastic Net Test RÂ²: {elastic_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count features selected by Elastic Net\n",
    "n_selected_elastic = (elastic_cv.coef_ != 0).sum()\n",
    "print(f\"Features selected by Elastic Net: {n_selected_elastic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS for baseline\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train_scaled, y_train)\n",
    "ols_pred = ols.predict(X_test_scaled)\n",
    "ols_rmse = np.sqrt(mean_squared_error(y_test, ols_pred))\n",
    "ols_r2 = r2_score(y_test, ols_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['OLS (Baseline)', 'Ridge', 'LASSO', 'Elastic Net'],\n",
    "    'Test RMSE': [ols_rmse, ridge_rmse, lasso_rmse, elastic_rmse],\n",
    "    'Test RÂ²': [ols_r2, ridge_r2, lasso_r2, elastic_r2],\n",
    "    'Features Used': [\n",
    "        len(feature_cols),\n",
    "        len(feature_cols),  # Ridge uses all\n",
    "        n_selected,\n",
    "        n_selected_elastic\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# RMSE comparison\n",
    "colors = ['gray', 'blue', 'green', 'orange']\n",
    "axes[0].bar(results['Model'], results['Test RMSE'], color=colors)\n",
    "axes[0].set_ylabel('Test RMSE')\n",
    "axes[0].set_title('Model Performance (Lower is Better)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Features used comparison\n",
    "axes[1].bar(results['Model'], results['Features Used'], color=colors)\n",
    "axes[1].set_ylabel('Number of Features')\n",
    "axes[1].set_title('Model Complexity (Features Used)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs Actual plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "models = [('OLS', ols_pred), ('Ridge', ridge_pred), ('LASSO', lasso_pred), ('Elastic Net', elastic_pred)]\n",
    "\n",
    "for ax, (name, pred) in zip(axes.flat, models):\n",
    "    ax.scatter(y_test, pred, alpha=0.5)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Actual Demand')\n",
    "    ax.set_ylabel('Predicted Demand')\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "1. **Created features** including some noise features\n",
    "2. **Applied Ridge regression** and saw all coefficients shrink\n",
    "3. **Applied LASSO regression** and saw automatic feature selection\n",
    "4. **Applied Elastic Net** combining both approaches\n",
    "5. **Compared performance** across all methods\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Method | What it does | When to use |\n",
    "|--------|--------------|-------------|\n",
    "| **Ridge** | Shrinks all coefficients | Many small/medium effects |\n",
    "| **LASSO** | Sets some coefficients to zero | Feature selection needed |\n",
    "| **Elastic Net** | Combines both | Correlated features + selection |\n",
    "\n",
    "### Connection to This Afternoon\n",
    "\n",
    "You now have a **LASSO baseline** to compare against:\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "\n",
    "These tree-based methods handle nonlinearities differently but also have their own regularization (max_depth, min_samples_leaf, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Tree-Based Methods notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
