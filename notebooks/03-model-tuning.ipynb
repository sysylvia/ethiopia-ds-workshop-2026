{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 (Part 2): Model Selection & Tuning\n",
    "\n",
    "**WISE Workshop | Addis Ababa, Feb 2026**\n",
    "\n",
    "In this notebook, you'll learn how to systematically improve your ML models through:\n",
    "- Cross-validation for robust evaluation\n",
    "- Hyperparameter tuning (grid search, random search)\n",
    "- Feature importance analysis\n",
    "- Final model selection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sysylvia/ethiopia-ds-workshop-2026/blob/main/notebooks/03-model-tuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    KFold\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Packages loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Data\n",
    "\n",
    "We'll use the same supply chain dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the supply chain dataset\n",
    "url = \"https://raw.githubusercontent.com/sysylvia/ethiopia-ds-workshop-2026/main/data/supply-chain-sample.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (same as notebook 02)\n",
    "# Encode categorical variables\n",
    "le_region = LabelEncoder()\n",
    "le_facility = LabelEncoder()\n",
    "le_season = LabelEncoder()\n",
    "\n",
    "df['region_encoded'] = le_region.fit_transform(df['region'])\n",
    "df['facility_encoded'] = le_facility.fit_transform(df['facility_type'])\n",
    "df['season_encoded'] = le_season.fit_transform(df['season'])\n",
    "\n",
    "# Define features\n",
    "feature_cols = [\n",
    "    'population_served', 'month', 'previous_demand', \n",
    "    'distance_to_warehouse', 'stockout_last_month', \n",
    "    'avg_delivery_days', 'storage_capacity',\n",
    "    'region_encoded', 'facility_encoded', 'season_encoded'\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['actual_demand']\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Why Tune Models?\n",
    "\n",
    "### Hyperparameters vs Learned Parameters\n",
    "\n",
    "| Type | What it is | Examples | How set? |\n",
    "|------|-----------|----------|----------|\n",
    "| **Learned parameters** | Values learned from data | Regression coefficients, tree splits | Training algorithm |\n",
    "| **Hyperparameters** | Choices that control learning | Number of trees, learning rate, max depth | You choose! |\n",
    "\n",
    "**Key insight:** Wrong hyperparameters can lead to underfitting (too simple) or overfitting (too complex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cross-Validation Deep Dive\n",
    "\n",
    "Cross-validation gives us a more robust estimate of model performance than a single train/test split.\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "```\n",
    "Fold 1: [TEST] [train] [train] [train] [train]\n",
    "Fold 2: [train] [TEST] [train] [train] [train]\n",
    "Fold 3: [train] [train] [TEST] [train] [train]\n",
    "Fold 4: [train] [train] [train] [TEST] [train]\n",
    "Fold 5: [train] [train] [train] [train] [TEST]\n",
    "\n",
    "Final score = average of all 5 folds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Random Forest with default settings\n",
    "rf_default = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    rf_default, X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Convert to RMSE (scores are negative MSE)\n",
    "cv_rmse = np.sqrt(-cv_scores)\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(f\"  RMSE per fold: {[f'{x:.2f}' for x in cv_rmse]}\")\n",
    "print(f\"  Mean RMSE: {cv_rmse.mean():.2f} (+/- {cv_rmse.std()*2:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "folds = range(1, 6)\n",
    "ax.bar(folds, cv_rmse, color='steelblue', alpha=0.7)\n",
    "ax.axhline(y=cv_rmse.mean(), color='red', linestyle='--', label=f'Mean RMSE: {cv_rmse.mean():.2f}')\n",
    "ax.fill_between([0.5, 5.5], \n",
    "                cv_rmse.mean() - cv_rmse.std(), \n",
    "                cv_rmse.mean() + cv_rmse.std(), \n",
    "                color='red', alpha=0.1, label='+/- 1 std')\n",
    "\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Cross-Validation Performance Across Folds')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Grid Search for Hyperparameter Tuning\n",
    "\n",
    "**Grid Search** tries every combination of hyperparameters you specify.\n",
    "\n",
    "For Random Forest, key hyperparameters include:\n",
    "- `n_estimators`: Number of trees (more = better but slower)\n",
    "- `max_depth`: Maximum tree depth (controls overfitting)\n",
    "- `min_samples_leaf`: Minimum samples in leaf nodes (prevents overly specific rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Total combinations\n",
    "total = len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_leaf'])\n",
    "print(f\"Total combinations to try: {total}\")\n",
    "print(f\"With 5-fold CV: {total * 5} model fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Running grid search (this may take a minute)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-grid_search.best_score_):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top 5 configurations\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results['rmse'] = np.sqrt(-results['mean_test_score'])\n",
    "\n",
    "top5 = results.nsmallest(5, 'rmse')[[\n",
    "    'param_n_estimators', 'param_max_depth', 'param_min_samples_leaf', \n",
    "    'rmse', 'rank_test_score'\n",
    "]]\n",
    "\n",
    "print(\"Top 5 Configurations:\")\n",
    "display(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Hyperparameter Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of max_depth on performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: max_depth effect\n",
    "depth_results = results.groupby('param_max_depth')['rmse'].mean().reset_index()\n",
    "depth_results['param_max_depth'] = depth_results['param_max_depth'].fillna('None')\n",
    "\n",
    "axes[0].bar(depth_results['param_max_depth'].astype(str), depth_results['rmse'], color='steelblue')\n",
    "axes[0].set_xlabel('max_depth')\n",
    "axes[0].set_ylabel('Mean CV RMSE')\n",
    "axes[0].set_title('Effect of max_depth')\n",
    "\n",
    "# Plot 2: n_estimators effect\n",
    "est_results = results.groupby('param_n_estimators')['rmse'].mean().reset_index()\n",
    "\n",
    "axes[1].bar(est_results['param_n_estimators'].astype(str), est_results['rmse'], color='coral')\n",
    "axes[1].set_xlabel('n_estimators')\n",
    "axes[1].set_ylabel('Mean CV RMSE')\n",
    "axes[1].set_title('Effect of n_estimators')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Random Search (Faster Alternative)\n",
    "\n",
    "When the hyperparameter space is large, **Random Search** samples random combinations instead of trying everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for random search\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'min_samples_split': randint(2, 20)\n",
    "}\n",
    "\n",
    "# Run random search with 20 iterations\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running random search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-random_search.best_score_):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Feature Importance Analysis\n",
    "\n",
    "Understanding which features matter helps:\n",
    "- Interpret the model\n",
    "- Identify important supply chain factors\n",
    "- Guide future data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Built-in feature importance (Mean Decrease in Impurity)\n",
    "importance_mdi = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (MDI - Built-in):\")\n",
    "display(importance_mdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance (more robust)\n",
    "perm_importance = permutation_importance(\n",
    "    best_model, X_test, y_test, \n",
    "    n_repeats=10, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_perm = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Permutation):\")\n",
    "display(importance_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both importance measures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MDI importance\n",
    "sns.barplot(data=importance_mdi, x='importance', y='feature', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Built-in Feature Importance (MDI)')\n",
    "axes[0].set_xlabel('Importance')\n",
    "\n",
    "# Permutation importance with error bars\n",
    "axes[1].barh(importance_perm['feature'], importance_perm['importance'], \n",
    "             xerr=importance_perm['std'], color='coral', alpha=0.7)\n",
    "axes[1].set_title('Permutation Feature Importance')\n",
    "axes[1].set_xlabel('Mean Importance')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Final Model Evaluation\n",
    "\n",
    "Now we evaluate the tuned model on the **held-out test set** (which we haven't touched during tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs tuned model on test set\n",
    "rf_default = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_default.fit(X_train, y_train)\n",
    "default_pred = rf_default.predict(X_test)\n",
    "\n",
    "tuned_pred = best_model.predict(X_test)\n",
    "\n",
    "# Metrics comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Default RF', 'Tuned RF'],\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_test, default_pred)),\n",
    "        np.sqrt(mean_squared_error(y_test, tuned_pred))\n",
    "    ],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(y_test, default_pred),\n",
    "        mean_absolute_error(y_test, tuned_pred)\n",
    "    ],\n",
    "    'R-squared': [\n",
    "        r2_score(y_test, default_pred),\n",
    "        r2_score(y_test, tuned_pred)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, pred, name in zip(axes, [default_pred, tuned_pred], ['Default RF', 'Tuned RF']):\n",
    "    ax.scatter(y_test, pred, alpha=0.5)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Actual Demand')\n",
    "    ax.set_ylabel('Predicted Demand')\n",
    "    ax.set_title(f'{name}\\nRMSE: {np.sqrt(mean_squared_error(y_test, pred)):.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Save Model for Day 3\n",
    "\n",
    "We'll save the trained model so we can use it in the ML-to-ABM integration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the tuned model\n",
    "model_info = {\n",
    "    'model': best_model,\n",
    "    'feature_cols': feature_cols,\n",
    "    'encoders': {\n",
    "        'region': le_region,\n",
    "        'facility_type': le_facility,\n",
    "        'season': le_season\n",
    "    },\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, tuned_pred))\n",
    "}\n",
    "\n",
    "# In Colab, save to session storage\n",
    "joblib.dump(model_info, 'demand_model.joblib')\n",
    "print(\"Model saved as 'demand_model.joblib'\")\n",
    "print(f\"\\nModel summary:\")\n",
    "print(f\"  Best parameters: {model_info['best_params']}\")\n",
    "print(f\"  Test RMSE: {model_info['test_rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Cross-validation** provides robust performance estimates\n",
    "2. **Grid search** systematically explores hyperparameter combinations\n",
    "3. **Random search** is faster for large search spaces\n",
    "4. **Feature importance** helps interpret model decisions\n",
    "5. **Final evaluation** on held-out test data prevents overfitting\n",
    "\n",
    "### Key Takeaways for Supply Chain Forecasting\n",
    "\n",
    "- Previous demand is typically the strongest predictor\n",
    "- Facility characteristics (type, capacity) capture structural differences\n",
    "- Seasonal patterns (rainy/dry) affect demand\n",
    "- Distance and delivery time influence supply chain dynamics\n",
    "\n",
    "**Next:** Use these predictions as inputs to Agent-Based Models (Day 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise (Optional)\n",
    "\n",
    "Try tuning a Gradient Boosting model using the same process:\n",
    "\n",
    "1. Define a parameter grid for `GradientBoostingRegressor` (try `n_estimators`, `learning_rate`, `max_depth`)\n",
    "2. Run grid search with 5-fold CV\n",
    "3. Compare the best GB model with the tuned RF model\n",
    "\n",
    "Which performs better on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: GradientBoostingRegressor key parameters:\n",
    "# - n_estimators: [50, 100, 200]\n",
    "# - learning_rate: [0.01, 0.1, 0.2]\n",
    "# - max_depth: [3, 5, 7]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
