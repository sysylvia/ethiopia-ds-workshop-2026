{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 1: Project Setup & ML Demo\n",
        "\n",
        "**WISE Workshop | Addis Ababa, Feb 2026**\n",
        "\n",
        "In this notebook, you'll:\n",
        "1. Set up a reproducible analysis project\n",
        "2. Explore the workshop dataset\n",
        "3. **See overfitting in action** with a sine wave demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check environment\n",
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Environment: {'Colab' if 'google.colab' in sys.modules else 'Local'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Settings\n",
        "pd.set_option('display.max_columns', 50)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Packages loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Load the Workshop Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load supply chain data from GitHub\n",
        "url = \"https://raw.githubusercontent.com/sysylvia/ethiopia-ds-workshop-2026/main/data/supply-chain-sample.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(url)\n",
        "    print(f\"Data loaded successfully! Shape: {df.shape}\")\n",
        "except:\n",
        "    # If URL not available, create sample data\n",
        "    print(\"Creating sample data for demonstration...\")\n",
        "    np.random.seed(42)\n",
        "    n_rows = 500\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'facility_id': np.random.choice(['ETH001', 'ETH002', 'ETH003', 'ETH004', 'ETH005'], n_rows),\n",
        "        'region': np.random.choice(['Addis Ababa', 'Oromia', 'Amhara', 'SNNP', 'Tigray'], n_rows),\n",
        "        'facility_type': np.random.choice(['Hospital', 'Health Center', 'Clinic'], n_rows, p=[0.2, 0.5, 0.3]),\n",
        "        'date': pd.date_range('2023-01-01', periods=n_rows, freq='D').strftime('%Y-%m'),\n",
        "        'medication_class': np.random.choice(['Antibiotics', 'Antimalarials', 'Chronic Disease', 'Vaccines', 'Other'], n_rows),\n",
        "        'demand': np.random.poisson(100, n_rows) + np.random.randint(0, 50, n_rows),\n",
        "        'stock_level': np.random.poisson(150, n_rows),\n",
        "        'lead_time_days': np.random.choice([7, 14, 21, 30], n_rows, p=[0.3, 0.4, 0.2, 0.1])\n",
        "    })\n",
        "    print(f\"Sample data created! Shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First look at the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data types and missing values\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check categorical variables\n",
        "for col in ['region', 'facility_type', 'medication_class']:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Initial Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of demand\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "df['demand'].hist(bins=30, ax=ax, edgecolor='black')\n",
        "ax.set_xlabel('Demand (units)')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Distribution of Demand')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demand by region\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "df.groupby('region')['demand'].mean().sort_values().plot(kind='barh', ax=ax)\n",
        "ax.set_xlabel('Average Demand')\n",
        "ax.set_ylabel('Region')\n",
        "ax.set_title('Average Demand by Region')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demand by facility type\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "sns.boxplot(data=df, x='facility_type', y='demand', ax=ax)\n",
        "ax.set_xlabel('Facility Type')\n",
        "ax.set_ylabel('Demand')\n",
        "ax.set_title('Demand Distribution by Facility Type')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Understanding Overfitting with the Sine Wave\n",
        "\n",
        "Now let's see the **bias-variance tradeoff** in action! We'll generate noisy data from a sine wave and try to fit polynomials of increasing complexity.\n",
        "\n",
        "**Key questions:**\n",
        "- When does the model fit too much noise?\n",
        "- Why does training error alone mislead us?\n",
        "- How do we find the right complexity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sine wave data with noise\n",
        "np.random.seed(42)\n",
        "\n",
        "n = 30  # number of data points\n",
        "X = np.sort(np.random.uniform(0, 2 * np.pi, n))  # random x values\n",
        "y_true = np.sin(X)  # the true underlying function\n",
        "y = y_true + np.random.normal(0, 0.3, n)  # add noise\n",
        "\n",
        "print(f\"Generated {n} noisy observations from sin(x)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the data and true function\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Plot true function\n",
        "x_smooth = np.linspace(0, 2 * np.pi, 100)\n",
        "ax.plot(x_smooth, np.sin(x_smooth), 'r--', linewidth=2, label='True function: sin(x)')\n",
        "\n",
        "# Plot noisy observations\n",
        "ax.scatter(X, y, s=60, c='blue', edgecolors='black', zorder=5, label='Observed data (noisy)')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_title('The Challenge: Recover the True Pattern from Noisy Data')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fitting Polynomials of Increasing Degree\n",
        "\n",
        "Let's fit polynomials with degrees 1 (linear), 3, 10, and 20 to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "degrees = [1, 3, 10, 20]\n",
        "colors = ['green', 'orange', 'purple', 'red']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot data and true function\n",
        "ax.scatter(X, y, s=60, c='blue', edgecolors='black', zorder=5, label='Data')\n",
        "ax.plot(x_smooth, np.sin(x_smooth), 'r--', linewidth=2, alpha=0.5, label='True: sin(x)')\n",
        "\n",
        "# Fit and plot each polynomial\n",
        "for degree, color in zip(degrees, colors):\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
        "    \n",
        "    # Fit linear regression on polynomial features\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    \n",
        "    # Predict on smooth x for plotting\n",
        "    X_smooth_poly = poly.transform(x_smooth.reshape(-1, 1))\n",
        "    y_pred_smooth = model.predict(X_smooth_poly)\n",
        "    \n",
        "    # Calculate training MSE\n",
        "    y_pred_train = model.predict(X_poly)\n",
        "    train_mse = mean_squared_error(y, y_pred_train)\n",
        "    \n",
        "    ax.plot(x_smooth, y_pred_smooth, color=color, linewidth=2, \n",
        "            label=f'Degree {degree} (Train MSE: {train_mse:.3f})')\n",
        "\n",
        "ax.set_xlabel('x', fontsize=12)\n",
        "ax.set_ylabel('y', fontsize=12)\n",
        "ax.set_title('Polynomial Fits of Increasing Complexity', fontsize=14)\n",
        "ax.legend(loc='upper right')\n",
        "ax.set_ylim(-2, 2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ¤” What do you notice?\n",
        "\n",
        "- **Degree 1 (green)**: Too simple! Misses the curve entirely.\n",
        "- **Degree 3 (orange)**: Captures the sine pattern reasonably well.\n",
        "- **Degree 10+ (purple, red)**: Starts wiggling through individual points.\n",
        "\n",
        "**But look at the training MSE!** Higher-degree polynomials have *lower* training error. Does that mean they're better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train vs Test Error: The Real Test\n",
        "\n",
        "Let's split our data and see what happens on **held-out test data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data: 70% train, 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare train vs test error across polynomial degrees\n",
        "degrees_to_test = range(1, 16)\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for degree in degrees_to_test:\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
        "    X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
        "    \n",
        "    # Fit model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    # Calculate errors\n",
        "    train_pred = model.predict(X_train_poly)\n",
        "    test_pred = model.predict(X_test_poly)\n",
        "    \n",
        "    train_errors.append(mean_squared_error(y_train, train_pred))\n",
        "    test_errors.append(mean_squared_error(y_test, test_pred))\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(degrees_to_test, train_errors, 'b-o', linewidth=2, markersize=8, label='Training Error')\n",
        "ax.plot(degrees_to_test, test_errors, 'r-s', linewidth=2, markersize=8, label='Test Error')\n",
        "\n",
        "# Find optimal degree\n",
        "optimal_degree = degrees_to_test[np.argmin(test_errors)]\n",
        "ax.axvline(x=optimal_degree, color='green', linestyle='--', alpha=0.7, \n",
        "           label=f'Optimal: Degree {optimal_degree}')\n",
        "\n",
        "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
        "ax.set_ylabel('Mean Squared Error', fontsize=12)\n",
        "ax.set_title('The Bias-Variance Tradeoff in Action', fontsize=14)\n",
        "ax.legend()\n",
        "ax.set_yscale('log')  # Log scale to see the pattern better\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ The \"Aha\" Moment!\n",
        "\n",
        "**Training error** keeps decreasing as we add complexity.\n",
        "\n",
        "**Test error** eventually starts INCREASING!\n",
        "\n",
        "This is **overfitting**: the model memorizes training data (including noise) and fails to generalize.\n",
        "\n",
        "---\n",
        "\n",
        "**Key lessons:**\n",
        "1. Training error alone is misleading\n",
        "2. We need held-out test data to evaluate models honestly\n",
        "3. More complex isn't always better\n",
        "4. The optimal complexity balances bias and variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Validation: A Better Approach\n",
        "\n",
        "Instead of a single train/test split, let's use **cross-validation** to get more stable estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Use cross-validation to find optimal degree\n",
        "degrees_to_test = range(1, 12)\n",
        "cv_scores = []\n",
        "\n",
        "for degree in degrees_to_test:\n",
        "    # Create pipeline: polynomial features + linear regression\n",
        "    pipeline = make_pipeline(\n",
        "        PolynomialFeatures(degree),\n",
        "        LinearRegression()\n",
        "    )\n",
        "    \n",
        "    # 5-fold cross-validation (negative MSE because sklearn maximizes)\n",
        "    scores = cross_val_score(pipeline, X.reshape(-1, 1), y, \n",
        "                            cv=5, scoring='neg_mean_squared_error')\n",
        "    cv_scores.append(-scores.mean())  # Convert back to positive MSE\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(degrees_to_test, cv_scores, 'g-o', linewidth=2, markersize=8)\n",
        "\n",
        "optimal_cv_degree = degrees_to_test[np.argmin(cv_scores)]\n",
        "ax.axvline(x=optimal_cv_degree, color='red', linestyle='--', alpha=0.7,\n",
        "           label=f'CV Optimal: Degree {optimal_cv_degree}')\n",
        "\n",
        "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
        "ax.set_ylabel('Cross-Validation MSE', fontsize=12)\n",
        "ax.set_title('Using Cross-Validation to Select Model Complexity', fontsize=14)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nâœ“ Cross-validation suggests degree {optimal_cv_degree} is optimal!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connection to Day 2: Regularization\n",
        "\n",
        "Instead of choosing polynomial degree, tomorrow we'll learn a more elegant approach:\n",
        "\n",
        "**LASSO and Ridge regression** add penalties that automatically constrain model complexity!\n",
        "\n",
        "```\n",
        "Today:     Choose degree to control complexity\n",
        "Tomorrow:  Use regularization penalty (Î») to control complexity\n",
        "```\n",
        "\n",
        "The same principle applies: **constrain complexity to prevent overfitting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: Save Your Work\n",
        "\n",
        "Don't forget to save a copy of this notebook to your Google Drive!\n",
        "\n",
        "**File > Save a copy in Drive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "1. âœ… Set up your Python environment\n",
        "2. âœ… Loaded and explored the workshop dataset\n",
        "3. âœ… Created initial visualizations\n",
        "4. âœ… **Saw overfitting in action** with the sine wave demo\n",
        "5. âœ… Learned why train/test splits and cross-validation matter\n",
        "\n",
        "**Key takeaway**: Complex models that fit training data perfectly often fail on new data. We need validation strategies to find the right balance.\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** Day 2 - Regularization (LASSO, Ridge) & Tree-Based Methods"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
