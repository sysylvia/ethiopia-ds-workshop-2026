{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Project Setup & ML Demo\n",
    "\n",
    "**WISE Workshop | Addis Ababa, Feb 2026**\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Set up a reproducible analysis project\n",
    "2. Explore the workshop dataset\n",
    "3. **See overfitting in action** with a sine wave demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Environment: {'Colab' if 'google.colab' in sys.modules else 'Local'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load the Workshop Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load supply chain data from GitHub\n",
    "url = \"https://raw.githubusercontent.com/sysylvia/ethiopia-ds-workshop-2026/main/data/supply-chain-sample.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(url)\n",
    "    print(f\"Data loaded successfully! Shape: {df.shape}\")\n",
    "except:\n",
    "    # If URL not available, create sample data matching the real data structure\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_rows = 500\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'facility_id': [f'F{str(i).zfill(3)}' for i in np.random.choice(range(1, 51), n_rows)],\n",
    "        'region': np.random.choice(['Addis Ababa', 'Oromia', 'Amhara', 'SNNP', 'Tigray'], n_rows),\n",
    "        'facility_type': np.random.choice(['Hospital', 'Health Center', 'Clinic'], n_rows, p=[0.2, 0.5, 0.3]),\n",
    "        'season': np.random.choice(['dry', 'rainy'], n_rows),\n",
    "        'month': np.random.choice(range(1, 13), n_rows),\n",
    "        'previous_demand': np.random.poisson(100, n_rows) + np.random.randint(0, 50, n_rows),\n",
    "        'actual_demand': np.random.poisson(100, n_rows) + np.random.randint(0, 50, n_rows),\n",
    "        'distance_to_warehouse': np.random.randint(10, 500, n_rows),\n",
    "        'avg_delivery_days': np.random.choice([3, 5, 7, 10, 14], n_rows)\n",
    "    })\n",
    "    print(f\"Sample data created! Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical variables\n",
    "for col in ['region', 'facility_type', 'season']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Initial Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of demand\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "df['actual_demand'].hist(bins=30, ax=ax, edgecolor='black')\n",
    "ax.set_xlabel('Demand (units)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Demand')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand by region\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df.groupby('region')['actual_demand'].mean().sort_values().plot(kind='barh', ax=ax)\n",
    "ax.set_xlabel('Average Demand')\n",
    "ax.set_ylabel('Region')\n",
    "ax.set_title('Average Demand by Region')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand by facility type\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.boxplot(data=df, x='facility_type', y='actual_demand', ax=ax)\n",
    "ax.set_xlabel('Facility Type')\n",
    "ax.set_ylabel('Demand')\n",
    "ax.set_title('Demand Distribution by Facility Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Understanding Overfitting with the Sine Wave\n",
    "\n",
    "Now let's see the **bias-variance tradeoff** in action! We'll generate noisy data from a sine wave and try to fit polynomials of increasing complexity.\n",
    "\n",
    "**Key questions:**\n",
    "- When does the model fit too much noise?\n",
    "- Why does training error alone mislead us?\n",
    "- How do we find the right complexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sine wave data with noise\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 30  # number of data points\n",
    "X = np.sort(np.random.uniform(0, 2 * np.pi, n))  # random x values\n",
    "y_true = np.sin(X)  # the true underlying function\n",
    "y = y_true + np.random.normal(0, 0.3, n)  # add noise\n",
    "\n",
    "print(f\"Generated {n} noisy observations from sin(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data and true function\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot true function\n",
    "x_smooth = np.linspace(0, 2 * np.pi, 100)\n",
    "ax.plot(x_smooth, np.sin(x_smooth), 'r--', linewidth=2, label='True function: sin(x)')\n",
    "\n",
    "# Plot noisy observations\n",
    "ax.scatter(X, y, s=60, c='blue', edgecolors='black', zorder=5, label='Observed data (noisy)')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('The Challenge: Recover the True Pattern from Noisy Data')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Population vs Sample: The Heart of Out-of-Sample Prediction\n",
    "\n",
    "In the real world, we only observe a **sample** from a larger **population**. Our goal is to learn patterns that generalize to the entire population\u2014not just memorize our sample.\n",
    "\n",
    "Let's make this concrete:\n",
    "- **Population**: 200 points from sin(x) + noise (imagine this is \"all possible data\")\n",
    "- **Sample**: We only get to see 30 of these points for training\n",
    "- **Test**: Evaluate how well our model predicts the other 170 points"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate the POPULATION: many more points\n",
    "np.random.seed(42)\n",
    "\n",
    "n_population = 200  # Full population\n",
    "n_sample = 30       # Our training sample\n",
    "\n",
    "# Generate population data\n",
    "X_pop = np.sort(np.random.uniform(0, 2 * np.pi, n_population))\n",
    "y_true_pop = np.sin(X_pop)\n",
    "y_pop = y_true_pop + np.random.normal(0, 0.3, n_population)\n",
    "\n",
    "# Take a random sample for training\n",
    "sample_idx = np.random.choice(n_population, n_sample, replace=False)\n",
    "sample_idx = np.sort(sample_idx)\n",
    "\n",
    "X_sample = X_pop[sample_idx]\n",
    "y_sample = y_pop[sample_idx]\n",
    "\n",
    "# The \"unseen\" population points (for testing)\n",
    "unseen_idx = np.setdiff1d(np.arange(n_population), sample_idx)\n",
    "X_unseen = X_pop[unseen_idx]\n",
    "y_unseen = y_pop[unseen_idx]\n",
    "\n",
    "print(f\"Population: {n_population} points\")\n",
    "print(f\"Sample (training): {n_sample} points\")\n",
    "print(f\"Unseen (testing): {len(unseen_idx)} points\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize population vs sample\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot unseen population (light gray)\n",
    "ax.scatter(X_unseen, y_unseen, s=40, c='lightgray', alpha=0.5,\n",
    "           label=f'Unseen population ({len(unseen_idx)} points)')\n",
    "\n",
    "# Plot training sample (blue)\n",
    "ax.scatter(X_sample, y_sample, s=80, c='blue', edgecolors='black',\n",
    "           zorder=5, label=f'Training sample ({n_sample} points)')\n",
    "\n",
    "# Plot true function\n",
    "ax.plot(x_smooth, np.sin(x_smooth), 'r--', linewidth=2, alpha=0.7,\n",
    "        label='True: sin(x)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Population vs Sample: We Only See the Blue Points!', fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWe train on the BLUE points, but want to predict the GRAY points well!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Polynomials of Increasing Degree\n",
    "\n",
    "Let's fit polynomials with degrees 1 (linear), 3, 10, and 20 to see what happens. We'll add each one step by step to watch the progression from underfitting to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and storage for fitted models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Store fitted models and predictions for cumulative plotting\n",
    "fitted_models = {}\n",
    "colors = {1: 'green', 3: 'orange', 10: 'purple', 20: 'red'}\n",
    "\n",
    "def fit_and_plot_polynomial(degree, fitted_models):\n",
    "    \"\"\"Fit a polynomial to the SAMPLE and plot all models fitted so far.\"\"\"\n",
    "    # Fit the new polynomial on SAMPLE data\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(X_sample.reshape(-1, 1))\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y_sample)\n",
    "    \n",
    "    # Calculate training MSE (on sample)\n",
    "    y_pred_train = model.predict(X_poly)\n",
    "    train_mse = mean_squared_error(y_sample, y_pred_train)\n",
    "    \n",
    "    # Store model info\n",
    "    X_smooth_poly = poly.transform(x_smooth.reshape(-1, 1))\n",
    "    y_pred_smooth = model.predict(X_smooth_poly)\n",
    "    fitted_models[degree] = {'predictions': y_pred_smooth, 'mse': train_mse, 'poly': poly, 'model': model}\n",
    "    \n",
    "    # Create plot with all models so far\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot unseen population in background (light gray)\n",
    "    ax.scatter(X_unseen, y_unseen, s=30, c='lightgray', alpha=0.3, label='Unseen population')\n",
    "    \n",
    "    # Plot sample data and true function\n",
    "    ax.scatter(X_sample, y_sample, s=60, c='blue', edgecolors='black', zorder=5, label='Training sample')\n",
    "    ax.plot(x_smooth, np.sin(x_smooth), 'r--', linewidth=2, alpha=0.5, label='True: sin(x)')\n",
    "    \n",
    "    # Plot all fitted models\n",
    "    for d in sorted(fitted_models.keys()):\n",
    "        info = fitted_models[d]\n",
    "        ax.plot(x_smooth, info['predictions'], color=colors[d], linewidth=2, \n",
    "                label=f'Degree {d} (Sample MSE: {info[\"mse\"]:.3f})')\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(f'Polynomial Fits on Sample Data (Now showing degree {degree})', fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylim(-2, 2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_mse\n",
    "\n",
    "print(\"Helper function ready. Let's fit polynomials one at a time!\")\n",
    "print(\"(Notice: We're training on the SAMPLE, with the unseen population shown in gray)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Degree 1: Linear fit (too simple?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit degree 1 polynomial (linear)\n",
    "mse = fit_and_plot_polynomial(1, fitted_models)\n",
    "print(f\"\\nDegree 1 (Linear): Training MSE = {mse:.3f}\")\n",
    "print(\"Notice: The line can't capture the curve at all! This is UNDERFITTING.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Degree 3: A cubic polynomial (getting better?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit degree 3 polynomial (cubic)\n",
    "mse = fit_and_plot_polynomial(3, fitted_models)\n",
    "print(f\"\\nDegree 3 (Cubic): Training MSE = {mse:.3f}\")\n",
    "print(\"Better! The cubic captures the wave pattern. MSE dropped significantly.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Degree 10: More flexibility (is more always better?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit degree 10 polynomial\n",
    "mse = fit_and_plot_polynomial(10, fitted_models)\n",
    "print(f\"\\nDegree 10: Training MSE = {mse:.3f}\")\n",
    "print(\"Hmm... MSE is even lower, but look at those wiggles! Is it fitting the data or the noise?\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Degree 20: Maximum flexibility (surely this is best?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit degree 20 polynomial\n",
    "mse = fit_and_plot_polynomial(20, fitted_models)\n",
    "print(f\"\\nDegree 20: Training MSE = {mse:.3f}\")\n",
    "print(\"Lowest training error yet! But look at those wild oscillations...\")\n",
    "print(\"This model is memorizing the noise, not learning the pattern. This is OVERFITTING!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Error vs Population Error: The Truth Revealed\n",
    "\n",
    "Now let's see what happens when we evaluate our polynomial models on:\n",
    "1. **Sample (training data)**: The 30 points we used to fit\n",
    "2. **Unseen population**: The 170 points we never saw\n",
    "\n",
    "This reveals the true cost of overfitting!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Compare sample error vs population error for different polynomial degrees\n",
    "degrees_to_compare = [1, 3, 10, 20]\n",
    "results = []\n",
    "\n",
    "for degree in degrees_to_compare:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_sample_poly = poly.fit_transform(X_sample.reshape(-1, 1))\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_sample_poly, y_sample)\n",
    "\n",
    "    # Error on sample (training)\n",
    "    y_sample_pred = model.predict(X_sample_poly)\n",
    "    sample_mse = mean_squared_error(y_sample, y_sample_pred)\n",
    "\n",
    "    # Error on unseen population\n",
    "    X_unseen_poly = poly.transform(X_unseen.reshape(-1, 1))\n",
    "    y_unseen_pred = model.predict(X_unseen_poly)\n",
    "    unseen_mse = mean_squared_error(y_unseen, y_unseen_pred)\n",
    "\n",
    "    results.append({\n",
    "        'Degree': degree,\n",
    "        'Sample MSE': round(sample_mse, 4),\n",
    "        'Unseen MSE': round(unseen_mse, 4)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize the gap between sample and population error\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_pos = np.arange(len(degrees_to_compare))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, results_df['Sample MSE'], width,\n",
    "               label='Sample (Training) MSE', color='blue', alpha=0.7)\n",
    "bars2 = ax.bar(x_pos + width/2, results_df['Unseen MSE'], width,\n",
    "               label='Unseen (Test) MSE', color='red', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax.set_title('The Overfitting Gap: Sample Error vs Population Error', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(degrees_to_compare)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: As degree increases, sample error drops but unseen error explodes!\")\n",
    "print(\"The gap between blue and red bars is the 'overfitting penalty'.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Population vs Sample: Key Takeaways\n",
    "\n",
    "1. **We only see a sample** of the true population\n",
    "2. **Training error** measures fit on the sample we saw\n",
    "3. **Test/population error** measures generalization to unseen data\n",
    "4. **Overfitting** = great on sample, terrible on population\n",
    "5. **The goal**: Find a model that performs well on *unseen* data\n",
    "\n",
    "This is why we need train/test splits, cross-validation, and regularization!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83e\udd14 What do you notice?\n",
    "\n",
    "- **Degree 1 (green)**: Too simple! Misses the curve entirely.\n",
    "- **Degree 3 (orange)**: Captures the sine pattern reasonably well.\n",
    "- **Degree 10+ (purple, red)**: Starts wiggling through individual points.\n",
    "\n",
    "**But look at the training MSE!** Higher-degree polynomials have *lower* training error. Does that mean they're better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train vs Test Error: The Real Test\n",
    "\n",
    "Let's split our data and see what happens on **held-out test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the SAMPLE data: 70% train, 30% test\n",
    "# (This mimics what we'd do in practice when we only have sample data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(\"\\n(Note: We're splitting our sample to simulate train/test validation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train vs test error across polynomial degrees\n",
    "degrees_to_test = range(1, 16)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees_to_test:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
    "    X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_pred = model.predict(X_train_poly)\n",
    "    test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    train_errors.append(mean_squared_error(y_train, train_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(degrees_to_test, train_errors, 'b-o', linewidth=2, markersize=8, label='Training Error')\n",
    "ax.plot(degrees_to_test, test_errors, 'r-s', linewidth=2, markersize=8, label='Test Error')\n",
    "\n",
    "# Find optimal degree\n",
    "optimal_degree = degrees_to_test[np.argmin(test_errors)]\n",
    "ax.axvline(x=optimal_degree, color='green', linestyle='--', alpha=0.7, \n",
    "           label=f'Optimal: Degree {optimal_degree}')\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax.set_title('The Bias-Variance Tradeoff in Action', fontsize=14)\n",
    "ax.legend()\n",
    "ax.set_yscale('log')  # Log scale to see the pattern better\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udca1 The \"Aha\" Moment!\n",
    "\n",
    "**Training error** keeps decreasing as we add complexity.\n",
    "\n",
    "**Test error** eventually starts INCREASING!\n",
    "\n",
    "This is **overfitting**: the model memorizes training data (including noise) and fails to generalize.\n",
    "\n",
    "---\n",
    "\n",
    "**Key lessons:**\n",
    "1. Training error alone is misleading\n",
    "2. We need held-out test data to evaluate models honestly\n",
    "3. More complex isn't always better\n",
    "4. The optimal complexity balances bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation: A Better Approach\n",
    "\n",
    "Instead of a single train/test split, let's use **cross-validation** to get more stable estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Use cross-validation on the SAMPLE to find optimal degree\n",
    "degrees_to_test = range(1, 12)\n",
    "cv_scores = []\n",
    "\n",
    "for degree in degrees_to_test:\n",
    "    # Create pipeline: polynomial features + linear regression\n",
    "    pipeline = make_pipeline(\n",
    "        PolynomialFeatures(degree),\n",
    "        LinearRegression()\n",
    "    )\n",
    "    \n",
    "    # 5-fold cross-validation (negative MSE because sklearn maximizes)\n",
    "    scores = cross_val_score(pipeline, X_sample.reshape(-1, 1), y_sample, \n",
    "                            cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores.append(-scores.mean())  # Convert back to positive MSE\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(degrees_to_test, cv_scores, 'g-o', linewidth=2, markersize=8)\n",
    "\n",
    "optimal_cv_degree = degrees_to_test[np.argmin(cv_scores)]\n",
    "ax.axvline(x=optimal_cv_degree, color='red', linestyle='--', alpha=0.7,\n",
    "           label=f'CV Optimal: Degree {optimal_cv_degree}')\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
    "ax.set_ylabel('Cross-Validation MSE', fontsize=12)\n",
    "ax.set_title('Using Cross-Validation to Select Model Complexity', fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u2713 Cross-validation suggests degree {optimal_cv_degree} is optimal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Day 2: Regularization\n",
    "\n",
    "Instead of choosing polynomial degree, tomorrow we'll learn a more elegant approach:\n",
    "\n",
    "**LASSO and Ridge regression** add penalties that automatically constrain model complexity!\n",
    "\n",
    "```\n",
    "Today:     Choose degree to control complexity\n",
    "Tomorrow:  Use regularization penalty (\u03bb) to control complexity\n",
    "```\n",
    "\n",
    "The same principle applies: **constrain complexity to prevent overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Save Your Work\n",
    "\n",
    "Don't forget to save a copy of this notebook to your Google Drive!\n",
    "\n",
    "**File > Save a copy in Drive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "1. \u2705 Set up your Python environment\n",
    "2. \u2705 Loaded and explored the workshop dataset\n",
    "3. \u2705 Created initial visualizations\n",
    "4. \u2705 **Saw overfitting in action** with the sine wave demo\n",
    "5. \u2705 Learned why train/test splits and cross-validation matter\n",
    "\n",
    "**Key takeaway**: Complex models that fit training data perfectly often fail on new data. We need validation strategies to find the right balance.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Day 2 - Regularization (LASSO, Ridge) & Tree-Based Methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}