{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 (Part 2): Model Selection & Tuning (R Version)\n",
    "\n",
    "**WISE Workshop | Addis Ababa, Feb 2026**\n",
    "\n",
    "In this notebook, you'll learn how to systematically improve your ML models through:\n",
    "- Cross-validation for robust evaluation\n",
    "- Hyperparameter tuning (grid search, random search)\n",
    "- Feature importance analysis\n",
    "- Final model selection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sysylvia/ethiopia-ds-workshop-2026/blob/main/notebooks/03-model-tuning-r.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "::: {.callout-important}\n",
    "## Google Colab R Runtime\n",
    "Make sure you're using the R runtime: **Runtime -> Change runtime type -> R**\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed (run once in Colab)\n",
    "if (!require(\"tidymodels\", quietly = TRUE)) {\n",
    "  install.packages(c(\"tidymodels\", \"ranger\", \"xgboost\", \"vip\"))\n",
    "}\n",
    "\n",
    "# Load packages\n",
    "library(tidymodels)\n",
    "library(tidyverse)\n",
    "library(vip)\n",
    "\n",
    "# Settings\n",
    "set.seed(42)\n",
    "theme_set(theme_minimal())\n",
    "\n",
    "cat(\"Packages loaded!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Data\n",
    "\n",
    "We'll use the same supply chain dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the supply chain dataset\n",
    "url <- \"https://raw.githubusercontent.com/sysylvia/ethiopia-ds-workshop-2026/main/data/supply-chain-sample.csv\"\n",
    "\n",
    "tryCatch({\n",
    "  df <- read_csv(url, show_col_types = FALSE)\n",
    "  cat(\"Data loaded from GitHub!\\n\")\n",
    "}, error = function(e) {\n",
    "  # Create sample data if URL not available\n",
    "  cat(\"Creating sample data...\\n\")\n",
    "  set.seed(42)\n",
    "  n <- 1000\n",
    "  \n",
    "  df <<- tibble(\n",
    "    region = sample(c('Addis Ababa', 'Oromia', 'Amhara', 'SNNP', 'Tigray'), n, replace = TRUE),\n",
    "    facility_type = sample(c('Hospital', 'Health Center', 'Clinic'), n, replace = TRUE, prob = c(0.2, 0.5, 0.3)),\n",
    "    season = sample(c('dry', 'rainy'), n, replace = TRUE),\n",
    "    population_served = round(runif(n, 5000, 100000)),\n",
    "    month = sample(1:12, n, replace = TRUE),\n",
    "    previous_demand = round(100 + rnorm(n, 0, 30)),\n",
    "    distance_to_warehouse = round(runif(n, 10, 500)),\n",
    "    stockout_last_month = sample(0:1, n, replace = TRUE, prob = c(0.8, 0.2)),\n",
    "    avg_delivery_days = round(runif(n, 3, 21)),\n",
    "    storage_capacity = round(runif(n, 500, 5000)),\n",
    "    actual_demand = round(100 + \n",
    "      ifelse(facility_type == 'Hospital', 80, ifelse(facility_type == 'Health Center', 40, 0)) +\n",
    "      0.3 * previous_demand + \n",
    "      rnorm(n, 0, 20))\n",
    "  )\n",
    "})\n",
    "\n",
    "cat(\"Data shape:\", nrow(df), \"rows x\", ncol(df), \"columns\\n\")\n",
    "cat(\"Columns:\", paste(names(df), collapse = \", \"), \"\\n\")\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "# Convert categorical to factors for proper handling\n",
    "df <- df %>%\n",
    "  mutate(\n",
    "    region = factor(region),\n",
    "    facility_type = factor(facility_type),\n",
    "    season = factor(season)\n",
    "  )\n",
    "\n",
    "cat(\"Features prepared\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "set.seed(42)\n",
    "data_split <- initial_split(df, prop = 0.8)\n",
    "train_data <- training(data_split)\n",
    "test_data <- testing(data_split)\n",
    "\n",
    "cat(\"Training set:\", nrow(train_data), \"samples\\n\")\n",
    "cat(\"Test set:\", nrow(test_data), \"samples\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Why Tune Models?\n",
    "\n",
    "### Hyperparameters vs Learned Parameters\n",
    "\n",
    "| Type | What it is | Examples | How set? |\n",
    "|------|-----------|----------|----------|\n",
    "| **Learned parameters** | Values learned from data | Regression coefficients, tree splits | Training algorithm |\n",
    "| **Hyperparameters** | Choices that control learning | Number of trees, learning rate, max depth | You choose! |\n",
    "\n",
    "**Key insight:** Wrong hyperparameters can lead to underfitting (too simple) or overfitting (too complex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cross-Validation Deep Dive\n",
    "\n",
    "Cross-validation gives us a more robust estimate of model performance than a single train/test split.\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "```\n",
    "Fold 1: [TEST] [train] [train] [train] [train]\n",
    "Fold 2: [train] [TEST] [train] [train] [train]\n",
    "Fold 3: [train] [train] [TEST] [train] [train]\n",
    "Fold 4: [train] [train] [train] [TEST] [train]\n",
    "Fold 5: [train] [train] [train] [train] [TEST]\n",
    "\n",
    "Final score = average of all 5 folds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recipe for tree models\n",
    "tree_recipe <- recipe(actual_demand ~ population_served + month + previous_demand + \n",
    "                                      distance_to_warehouse + stockout_last_month + \n",
    "                                      avg_delivery_days + storage_capacity + \n",
    "                                      region + facility_type + season, \n",
    "                     data = train_data)\n",
    "\n",
    "# Baseline: Random Forest with default settings\n",
    "rf_default_spec <- rand_forest() %>%\n",
    "  set_engine(\"ranger\") %>%\n",
    "  set_mode(\"regression\")\n",
    "\n",
    "rf_default_wf <- workflow() %>%\n",
    "  add_recipe(tree_recipe) %>%\n",
    "  add_model(rf_default_spec)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "set.seed(42)\n",
    "cv_folds <- vfold_cv(train_data, v = 5)\n",
    "\n",
    "# Fit and evaluate across folds\n",
    "cv_results <- fit_resamples(\n",
    "  rf_default_wf,\n",
    "  resamples = cv_folds,\n",
    "  metrics = metric_set(rmse, rsq)\n",
    ")\n",
    "\n",
    "# Collect metrics\n",
    "cv_metrics <- collect_metrics(cv_results)\n",
    "\n",
    "cat(\"5-Fold Cross-Validation Results:\\n\")\n",
    "print(cv_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results per fold\n",
    "cv_rmse_per_fold <- cv_results %>%\n",
    "  collect_metrics(summarize = FALSE) %>%\n",
    "  filter(.metric == \"rmse\")\n",
    "\n",
    "mean_rmse <- mean(cv_rmse_per_fold$.estimate)\n",
    "sd_rmse <- sd(cv_rmse_per_fold$.estimate)\n",
    "\n",
    "ggplot(cv_rmse_per_fold, aes(x = id, y = .estimate)) +\n",
    "  geom_col(fill = \"steelblue\", alpha = 0.7) +\n",
    "  geom_hline(yintercept = mean_rmse, color = \"red\", linetype = \"dashed\") +\n",
    "  geom_ribbon(aes(ymin = mean_rmse - sd_rmse, ymax = mean_rmse + sd_rmse, group = 1), \n",
    "              fill = \"red\", alpha = 0.1) +\n",
    "  labs(\n",
    "    x = \"Fold\",\n",
    "    y = \"RMSE\",\n",
    "    title = \"Cross-Validation Performance Across Folds\",\n",
    "    subtitle = sprintf(\"Mean RMSE: %.2f (+/- %.2f)\", mean_rmse, sd_rmse)\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Grid Search for Hyperparameter Tuning\n",
    "\n",
    "**Grid Search** tries every combination of hyperparameters you specify.\n",
    "\n",
    "For Random Forest, key hyperparameters include:\n",
    "- `trees`: Number of trees (more = better but slower)\n",
    "- `mtry`: Number of features to consider at each split\n",
    "- `min_n`: Minimum samples in leaf nodes (prevents overly specific rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tunable Random Forest specification\n",
    "rf_tune_spec <- rand_forest(\n",
    "  trees = tune(),\n",
    "  mtry = tune(),\n",
    "  min_n = tune()\n",
    ") %>%\n",
    "  set_engine(\"ranger\", importance = \"impurity\") %>%\n",
    "  set_mode(\"regression\")\n",
    "\n",
    "rf_tune_wf <- workflow() %>%\n",
    "  add_recipe(tree_recipe) %>%\n",
    "  add_model(rf_tune_spec)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_grid <- grid_regular(\n",
    "  trees(range = c(50, 200)),\n",
    "  mtry(range = c(2, 8)),\n",
    "  min_n(range = c(2, 10)),\n",
    "  levels = 3\n",
    ")\n",
    "\n",
    "cat(\"Total combinations to try:\", nrow(rf_grid), \"\\n\")\n",
    "cat(\"With 5-fold CV:\", nrow(rf_grid) * 5, \"model fits\\n\")\n",
    "print(head(rf_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "cat(\"Running grid search (this may take a minute)...\\n\")\n",
    "\n",
    "set.seed(42)\n",
    "rf_tune_results <- tune_grid(\n",
    "  rf_tune_wf,\n",
    "  resamples = cv_folds,\n",
    "  grid = rf_grid,\n",
    "  metrics = metric_set(rmse, rsq)\n",
    ")\n",
    "\n",
    "# Show best results\n",
    "cat(\"\\nTop 5 Configurations:\\n\")\n",
    "show_best(rf_tune_results, metric = \"rmse\", n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters\n",
    "best_rf_params <- select_best(rf_tune_results, metric = \"rmse\")\n",
    "cat(\"Best parameters:\\n\")\n",
    "print(best_rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Hyperparameter Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tuning results\n",
    "autoplot(rf_tune_results) +\n",
    "  theme_minimal() +\n",
    "  labs(title = \"Hyperparameter Tuning Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of trees on performance\n",
    "tune_metrics <- collect_metrics(rf_tune_results) %>%\n",
    "  filter(.metric == \"rmse\")\n",
    "\n",
    "ggplot(tune_metrics, aes(x = trees, y = mean, color = factor(min_n))) +\n",
    "  geom_line() +\n",
    "  geom_point() +\n",
    "  facet_wrap(~mtry, labeller = label_both) +\n",
    "  labs(\n",
    "    x = \"Number of Trees\",\n",
    "    y = \"Mean CV RMSE\",\n",
    "    color = \"min_n\",\n",
    "    title = \"Effect of Hyperparameters on Performance\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Random Search (Faster Alternative)\n",
    "\n",
    "When the hyperparameter space is large, **Random Search** samples random combinations instead of trying everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a larger search space for random search\n",
    "rf_random_grid <- grid_random(\n",
    "  trees(range = c(50, 300)),\n",
    "  mtry(range = c(2, 10)),\n",
    "  min_n(range = c(2, 20)),\n",
    "  size = 20  # Only try 20 random combinations\n",
    ")\n",
    "\n",
    "cat(\"Random search will try\", nrow(rf_random_grid), \"combinations\\n\")\n",
    "\n",
    "# Run random search\n",
    "set.seed(42)\n",
    "rf_random_results <- tune_grid(\n",
    "  rf_tune_wf,\n",
    "  resamples = cv_folds,\n",
    "  grid = rf_random_grid,\n",
    "  metrics = metric_set(rmse, rsq)\n",
    ")\n",
    "\n",
    "cat(\"\\nBest from random search:\\n\")\n",
    "show_best(rf_random_results, metric = \"rmse\", n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Feature Importance Analysis\n",
    "\n",
    "Understanding which features matter helps:\n",
    "- Interpret the model\n",
    "- Identify important supply chain factors\n",
    "- Guide future data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and fit the best model\n",
    "final_rf_wf <- rf_tune_wf %>%\n",
    "  finalize_workflow(best_rf_params)\n",
    "\n",
    "final_rf_fit <- final_rf_wf %>%\n",
    "  fit(data = train_data)\n",
    "\n",
    "# Feature importance using vip\n",
    "final_rf_fit %>%\n",
    "  extract_fit_parsnip() %>%\n",
    "  vip(num_features = 10) +\n",
    "  labs(title = \"Feature Importance (Tuned Random Forest)\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importance values as a table\n",
    "importance_df <- final_rf_fit %>%\n",
    "  extract_fit_parsnip() %>%\n",
    "  vi() %>%\n",
    "  arrange(desc(Importance))\n",
    "\n",
    "cat(\"Feature Importance Table:\\n\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Final Model Evaluation\n",
    "\n",
    "Now we evaluate the tuned model on the **held-out test set** (which we haven't touched during tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs tuned model on test set\n",
    "\n",
    "# Fit default model\n",
    "default_fit <- rf_default_wf %>% fit(data = train_data)\n",
    "default_pred <- predict(default_fit, test_data) %>%\n",
    "  bind_cols(test_data %>% select(actual_demand))\n",
    "\n",
    "# Predict with tuned model\n",
    "tuned_pred <- predict(final_rf_fit, test_data) %>%\n",
    "  bind_cols(test_data %>% select(actual_demand))\n",
    "\n",
    "# Calculate metrics\n",
    "default_metrics <- default_pred %>% metrics(truth = actual_demand, estimate = .pred)\n",
    "tuned_metrics <- tuned_pred %>% metrics(truth = actual_demand, estimate = .pred)\n",
    "\n",
    "# Comparison table\n",
    "comparison <- tibble(\n",
    "  Model = c('Default RF', 'Tuned RF'),\n",
    "  RMSE = c(\n",
    "    default_metrics %>% filter(.metric == \"rmse\") %>% pull(.estimate),\n",
    "    tuned_metrics %>% filter(.metric == \"rmse\") %>% pull(.estimate)\n",
    "  ),\n",
    "  MAE = c(\n",
    "    default_metrics %>% filter(.metric == \"mae\") %>% pull(.estimate),\n",
    "    tuned_metrics %>% filter(.metric == \"mae\") %>% pull(.estimate)\n",
    "  ),\n",
    "  R_squared = c(\n",
    "    default_metrics %>% filter(.metric == \"rsq\") %>% pull(.estimate),\n",
    "    tuned_metrics %>% filter(.metric == \"rsq\") %>% pull(.estimate)\n",
    "  )\n",
    ") %>%\n",
    "  mutate(across(where(is.numeric), ~round(., 3)))\n",
    "\n",
    "cat(\"Test Set Performance:\\n\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "all_preds <- bind_rows(\n",
    "  default_pred %>% mutate(Model = \"Default RF\"),\n",
    "  tuned_pred %>% mutate(Model = \"Tuned RF\")\n",
    ")\n",
    "\n",
    "ggplot(all_preds, aes(x = actual_demand, y = .pred)) +\n",
    "  geom_point(alpha = 0.5) +\n",
    "  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n",
    "  facet_wrap(~Model) +\n",
    "  labs(\n",
    "    x = \"Actual Demand\",\n",
    "    y = \"Predicted Demand\",\n",
    "    title = \"Default vs Tuned Random Forest\"\n",
    "  ) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Save Model for Day 3\n",
    "\n",
    "We'll save the trained model so we can use it in the ML-to-ABM integration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned workflow (includes preprocessing and model)\n",
    "saveRDS(final_rf_fit, \"demand_model.rds\")\n",
    "\n",
    "# Also save model info\n",
    "model_info <- list(\n",
    "  best_params = best_rf_params,\n",
    "  test_rmse = tuned_metrics %>% filter(.metric == \"rmse\") %>% pull(.estimate),\n",
    "  feature_importance = importance_df\n",
    ")\n",
    "saveRDS(model_info, \"demand_model_info.rds\")\n",
    "\n",
    "cat(\"Model saved as 'demand_model.rds'\\n\")\n",
    "cat(\"\\nModel summary:\\n\")\n",
    "cat(\"  Best parameters: trees =\", best_rf_params$trees, \n",
    "    \", mtry =\", best_rf_params$mtry, \n",
    "    \", min_n =\", best_rf_params$min_n, \"\\n\")\n",
    "cat(\"  Test RMSE:\", round(model_info$test_rmse, 2), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Cross-validation** (`vfold_cv()`) provides robust performance estimates\n",
    "2. **Grid search** (`tune_grid()` + `grid_regular()`) systematically explores hyperparameter combinations\n",
    "3. **Random search** (`grid_random()`) is faster for large search spaces\n",
    "4. **Feature importance** (`vip()`) helps interpret model decisions\n",
    "5. **Final evaluation** on held-out test data prevents overfitting\n",
    "\n",
    "### Key tidymodels Functions for Tuning\n",
    "\n",
    "| Task | Function |\n",
    "|------|----------|\n",
    "| Mark parameter for tuning | `tune()` |\n",
    "| Create CV folds | `vfold_cv()` |\n",
    "| Create parameter grid | `grid_regular()`, `grid_random()` |\n",
    "| Tune model | `tune_grid()` |\n",
    "| Get best parameters | `select_best()` |\n",
    "| Finalize workflow | `finalize_workflow()` |\n",
    "| Feature importance | `vip()`, `vi()` |\n",
    "\n",
    "### Key Takeaways for Supply Chain Forecasting\n",
    "\n",
    "- Previous demand is typically the strongest predictor\n",
    "- Facility characteristics (type, capacity) capture structural differences\n",
    "- Seasonal patterns (rainy/dry) affect demand\n",
    "- Distance and delivery time influence supply chain dynamics\n",
    "\n",
    "**Next:** Use these predictions as inputs to Agent-Based Models (Day 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise (Optional)\n",
    "\n",
    "Try tuning a Gradient Boosting model using the same process:\n",
    "\n",
    "1. Define a parameter grid for `boost_tree()` (try `trees`, `learn_rate`, `tree_depth`)\n",
    "2. Run grid search with 5-fold CV\n",
    "3. Compare the best GB model with the tuned RF model\n",
    "\n",
    "Which performs better on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: boost_tree key parameters:\n",
    "# - trees: 50, 100, 200\n",
    "# - learn_rate: 0.01, 0.1, 0.3\n",
    "# - tree_depth: 3, 5, 7\n",
    "\n",
    "# gb_spec <- boost_tree(\n",
    "#   trees = tune(),\n",
    "#   learn_rate = tune(),\n",
    "#   tree_depth = tune()\n",
    "# ) %>%\n",
    "#   set_engine(\"xgboost\") %>%\n",
    "#   set_mode(\"regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
