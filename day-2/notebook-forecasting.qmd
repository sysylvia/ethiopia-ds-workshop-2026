---
title: "Hands-on: Demand Forecasting"
subtitle: "Day 2 Afternoon | Full ML Pipeline with Trees & Ensembles"
---

## Open in Google Colab

Click the badge below to open this notebook directly in Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sysylvia/ethiopia-ds-workshop-2026/blob/main/notebooks/03-demand-forecasting.ipynb)

::: {.callout-tip}
## Save Your Work
After opening, go to **File > Save a copy in Drive** to save your progress.
:::

## Learning Objectives

In this hands-on session, you will:

1. Build a complete ML pipeline for demand forecasting
2. Train and compare Decision Trees, Random Forest, and Gradient Boosting
3. Use cross-validation for honest model comparison
4. Analyze feature importance across models
5. Generate forecasts for use in Days 3-4 simulation

## The Problem

You are a data scientist working with the Ethiopian pharmaceutical supply chain. Your task is to build a model that predicts **demand** for essential medications based on:

- Historical consumption data
- Facility characteristics
- Seasonal patterns
- Regional factors

These predictions will feed into the **agent-based models** on Days 3-4.

## Exercise Overview

### Part 1: Data Preparation (15 min)

- Load the supply chain dataset
- Create features (lags, dummies)
- Train/test split

### Part 2: Decision Tree (15 min)

- Fit a single decision tree
- Visualize the tree structure
- See how it captures nonlinearities
- Observe overfitting with deep trees

### Part 3: Random Forest (25 min)

- Fit Random Forest with different parameters
- Compare to single tree performance
- Use OOB error for quick validation
- Examine feature importance

### Part 4: Gradient Boosting (25 min)

- Fit Gradient Boosting (XGBoost or sklearn)
- Tune key parameters: learning_rate, n_estimators, max_depth
- Cross-validate to prevent overfitting
- Compare to Random Forest

### Part 5: Model Comparison (20 min)

- Compare all models: LASSO (from morning), RF, GB
- Cross-validated RMSE for each
- Feature importance comparison
- Select best model for simulation

## Key Concepts from Lecture

| Concept | What You'll See |
|---------|-----------------|
| Trees capture interactions | Tree splits show feature combinations |
| RF reduces variance | Smoother predictions than single tree |
| GB learns from mistakes | Sequential improvement in residuals |
| Feature importance | Which features drive predictions |

## Code Snippets

### Random Forest

```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    oob_score=True
)
rf.fit(X_train, y_train)
print(f"OOB Score: {rf.oob_score_:.3f}")
```

### Gradient Boosting

```python
from sklearn.ensemble import GradientBoostingRegressor

gb = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)
gb.fit(X_train, y_train)
```

### Feature Importance

```python
import pandas as pd

importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

importance.head(10).plot(kind='barh', x='feature', y='importance')
```

## Connection to Days 3-4

The forecasts you generate here become **inputs to the ABM simulation**:

```
Day 2 Output → Days 3-4 Input
─────────────────────────────
Demand forecasts → Agent ordering behavior
Feature importance → What-if scenarios
Model predictions → Inventory planning
```

## After This Session

You will have:

- Built and compared multiple ML models
- Understanding of the full ML pipeline
- A trained model for demand forecasting
- Feature importance insights for supply chain decisions
- Predictions ready for simulation on Days 3-4
